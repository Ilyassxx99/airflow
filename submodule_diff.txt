Submodule client1 0000000...4f10ef1 (new submodule)
Submodule client2 0000000...76ca5f9 (new submodule)
diff --git a/data-engineering-use-case/analyze_customer_feedback.py b/data-engineering-use-case/analyze_customer_feedback.py
deleted file mode 100644
index afb7d3a..0000000
--- a/data-engineering-use-case/analyze_customer_feedback.py
+++ /dev/null
@@ -1,133 +0,0 @@
-"""
-## Analyzes our customer feedback 
-Our customers only deserve the best toys!
-Remember, these feedback comments are only a proxy since
-they were submitted by our customer's humans.
-![A very good dog](https://place.dog/300/200)
-"""
-
-from airflow.datasets import Dataset
-from airflow.decorators import dag, task_group, task
-from airflow.models.baseoperator import chain
-from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
-from pendulum import datetime
-import string
-from include.helpers import deep_getsizeof
-
-
-SNOWFLAKE_CONN_ID = "snowflake_de_team"
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule=[Dataset("snowflake://customer_feedback_table")],
-    catchup=False,
-    max_consecutive_failed_dag_runs=5,  # NEW in Airflow 2.9: Pause the DAG after x failed runs
-    tags=[
-        "Max Consecutive Failed Runs",
-        "Object Store custom XCom backend",
-        "use-case",
-    ],
-    default_args={"owner": "Pakkun", "retries": 3, "retry_delay": 5},
-    description="Analyze customer feedback",
-    doc_md=__doc__,
-)
-def analyze_toy_feedback():
-    gather_feedback = SnowflakeOperator(
-        task_id=f"gather_feedback",
-        snowflake_conn_id=SNOWFLAKE_CONN_ID,
-        sql="""
-        SELECT DISTINCT(COMMENTS) FROM customer_feedback_table;
-        """,
-    )
-
-    @task_group
-    def get_toy_sentiment(feedback):
-        # NEW in Airflow 2.9: Define custom names for the map index
-        @task(map_index_template="{{ my_mapping_variable }}")
-        def process_feedback(feedback):
-            from airflow.operators.python import get_current_context
-
-            context = get_current_context()
-            comment = feedback["COMMENTS"].translate(
-                str.maketrans("", "", string.punctuation)
-            )
-            context["my_mapping_variable"] = f"{comment}"
-            return comment
-
-        @task(queue="ml-queue", map_index_template="{{ my_mapping_variable }}")
-        # NEW in Airflow 2.9: Define custom names for the map index
-        def analyze_sentiment(processed_text):
-            from airflow.operators.python import get_current_context
-            from transformers import pipeline
-
-            context = get_current_context()
-
-            sentiment_pipeline = pipeline(
-                "sentiment-analysis",
-                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
-            )
-            sentence = processed_text
-            result = sentiment_pipeline(sentence)
-            context["my_mapping_variable"] = (
-                f"{sentence} - {result[0]['label']} - {round(result[0]['score'], 3)}"
-            )
-            print(result)
-            return result
-
-        @task(multiple_outputs=True, map_index_template="{{ my_mapping_variable }}")
-        # NEW in Airflow 2.9: Define custom names for the map index
-        def create_embeddings(comment):
-            from airflow.operators.python import get_current_context
-            from sys import getsizeof
-            from transformers import pipeline
-
-            context = get_current_context()
-
-            embedding_pipeline = pipeline(
-                "feature-extraction",
-                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
-            )
-
-            embeddings = embedding_pipeline(comment)
-            print(f"Size of comment: {getsizeof(comment.encode('utf-8'))} bytes")
-            print(f"Size of embeddings: {deep_getsizeof(embeddings, set())} bytes")
-
-            context["my_mapping_variable"] = f"{comment}"
-
-            # NEW in Airflow 2.9: Use Object Storage custom XCom backend to send
-            # large XCom values to an object storage container
-            return {"embeddings": embeddings, "comment": comment}
-
-        processed_feedback = process_feedback(feedback)
-
-        create_embeddings(processed_feedback)
-
-        return analyze_sentiment(processed_feedback)
-
-    @task
-    def report_on_results(sentiments):
-        positive_count = 0
-        negative_count = 0
-        neutral_count = 0
-        for sentiment in sentiments:
-            label = sentiment[0]["label"]
-            if label == "positive":
-                positive_count += 1
-            elif label == "negative":
-                negative_count += 1
-            else:
-                neutral_count += 1
-
-        print(
-            f"Positive: {positive_count}, Negative: {negative_count}, Neutral: {neutral_count}"
-        )
-        return positive_count, negative_count, neutral_count
-
-    sentiments = get_toy_sentiment.expand(feedback=gather_feedback.output)
-    report = report_on_results(sentiments)
-
-    chain(sentiments, report)
-
-
-analyze_toy_feedback()
diff --git a/data-engineering-use-case/ingestion/create_ingestion_dags.py b/data-engineering-use-case/ingestion/create_ingestion_dags.py
deleted file mode 100644
index c4867e9..0000000
--- a/data-engineering-use-case/ingestion/create_ingestion_dags.py
+++ /dev/null
@@ -1,218 +0,0 @@
-"""
-## Dynamically generated DAGs for ingestion
-
-This DAG is generated dynamically from the `ingestion_source_config.json` file.
-There is one DAG per source.
-"""
-
-from airflow.datasets import Dataset
-from airflow.decorators import dag, task
-from airflow.models.baseoperator import chain
-from airflow.sensors.base import PokeReturnValue
-from airflow.io.path import ObjectStoragePath
-from airflow.operators.empty import EmptyOperator
-from pendulum import parse, datetime, duration
-from typing import Any
-import json
-from include.ingestion.ingest_functions import evaluate_new_file, verify_checksum
-
-
-def create_ingest_dags(
-    source_name: str,
-    base_path_ingest: ObjectStoragePath,
-    conn_id_ingest: str,
-    base_path_intermediate: ObjectStoragePath,
-    conn_id_intermediate: str,
-    base_path_load: ObjectStoragePath,
-    conn_id_load: str,
-    dataset_uri: str,
-    dag_id: str,
-    start_date: datetime = None,
-    schedule: Any = None,
-    catchup: bool = False,
-    tags: list = ["NONE"],
-    dag_owner: str = "airflow",
-    task_retries: int = 3,
-    task_retry_delay: duration = duration(minutes=5),
-):
-    @dag(
-        dag_id=dag_id,
-        start_date=start_date,
-        schedule=schedule,
-        catchup=catchup,
-        tags=tags,
-        default_args={
-            "owner": dag_owner,
-            "retries": task_retries,
-            "retry_delay": task_retry_delay,
-        },
-        description=f"Ingest data from {source_name}",
-        doc_md=__doc__,
-    )
-    def ingest_dag():
-        @task.sensor(
-            task_id=f"wait_for_new_files_{source_name}",
-            doc="This task waits for new files to arrive in the source bucket.",
-            poke_interval=30,
-            timeout=3600,
-            mode="poke",
-        )
-        def wait_for_new_files(
-            base_path: ObjectStoragePath, source_name: str, conn_id_ingest: str
-        ) -> PokeReturnValue:
-            """Wait for a new file to arrive in the source satisfying given criteria."""
-            path = ObjectStoragePath(
-                f"{base_path}/{source_name}", conn_id=conn_id_ingest
-            )
-
-            files = [f for f in path.iterdir() if f.is_file()]
-            is_condition_met = evaluate_new_file(files)
-
-            return PokeReturnValue(is_done=is_condition_met, xcom_value=files)
-
-        @task(
-            task_id=f"extract_{source_name}",
-            map_index_template="{{ my_mapping_variable }}",
-        )
-        def extract(
-            base_path_intermediate: ObjectStoragePath,
-            conn_id_intermediate: str,
-            source_name: str,
-            source_file: ObjectStoragePath,
-        ) -> list:
-            """Extract data from source and write it to intermediary storage."""
-            from airflow.operators.python import get_current_context
-
-            file_name = source_file.name
-
-            # NEW in Airflow 2.9: custom index for dynamically mapped task instances
-            context = get_current_context()
-            context["my_mapping_variable"] = f"Source file: {file_name}"
-
-            print(f"Extracting {source_file} and copy to {base_path_intermediate}.")
-
-            intermediate_file_loc = ObjectStoragePath(
-                f"{base_path_intermediate}{source_name}/{file_name}",
-                conn_id=conn_id_intermediate,
-            )
-
-            source_file.copy(dst=intermediate_file_loc)
-
-            return intermediate_file_loc
-
-        @task(
-            task_id=f"verify_checksum_{source_name}",
-            map_index_template="{{ my_mapping_variable }}",
-        )
-        def check_checksum(
-            file: ObjectStoragePath,
-        ) -> list:
-            from airflow.operators.python import get_current_context
-
-            file_name = file.name
-
-            # NEW in Airflow 2.9: custom index for dynamically mapped task instances
-            context = get_current_context()
-            context["my_mapping_variable"] = f"Source file: {file_name}"
-            check_sum_file = file.checksum()
-
-            result = verify_checksum(check_sum_file)
-
-            return result
-
-        @task(
-            task_id=f"load_{source_name}",
-            map_index_template="{{ my_mapping_variable }}",
-        )
-        def load(
-            source_file: ObjectStoragePath,
-            base_path_load: ObjectStoragePath,
-            conn_id_load: str,
-        ) -> list:
-            "Load data from intermediary to load storage."
-            from airflow.operators.python import get_current_context
-
-            file_name = source_file.name
-
-            # NEW in Airflow 2.9: custom index for dynamically mapped task instances
-            context = get_current_context()
-            context["my_mapping_variable"] = f"Source file: {file_name}"
-
-            print(f"Extracting {source_file} and writing it to {base_path_load}.")
-
-            load_file_loc = ObjectStoragePath(
-                f"{base_path_load}{source_name}/{file_name}",
-                conn_id=conn_id_load,
-            )
-
-            source_file.copy(dst=load_file_loc)
-
-            return load_file_loc
-
-        update_dataset_obj = EmptyOperator(
-            task_id=f"update_dataset_{source_name}", outlets=[Dataset(dataset_uri)]
-        )
-
-        # Calling TaskFlow tasks, inferring dependencies
-        source_files = wait_for_new_files(base_path_ingest, source_name, conn_id_ingest)
-
-        # Dynamically map the extract, transform and load tasks over the list of new file locations
-        extract_obj = extract.partial(
-            base_path_intermediate=base_path_intermediate,
-            conn_id_intermediate=conn_id_intermediate,
-            source_name=source_name,
-        ).expand(source_file=source_files)
-
-        checked_files = check_checksum.expand(file=extract_obj)
-
-        load_obj = load.partial(
-            base_path_load=base_path_load,
-            conn_id_load=conn_id_load,
-        ).expand(source_file=extract_obj)
-
-        # Define dependencies explicitly
-        chain(checked_files, load_obj, update_dataset_obj)
-
-    ingest_dag_obj = ingest_dag()
-
-    return ingest_dag_obj
-
-
-with open("include/ingestion_source_config.json", "r") as f:
-    config = json.load(f)
-
-
-for source in config["sources"]:
-    source_name = source["source_name"]
-    base_path_ingest = ObjectStoragePath(source["base_path_ingest"])
-    conn_id_ingest = source["conn_id_ingest"]
-    base_path_intermediate = ObjectStoragePath(source["base_path_intermediate"])
-    conn_id_intermediate = source["conn_id_intermediate"]
-    base_path_load = source["base_path_load"]
-    conn_id_load = source["conn_id_load"]
-    dataset_uri = source["dataset_uri"]
-    dag_id = source["dag_id"]
-    start_date = parse(source["start_date"])
-    schedule = source["schedule"]
-    catchup = source["catchup"]
-    tags = source["tags"]
-    dag_owner = source["dag_owner"]
-    task_retries = source["task_retries"]
-
-    globals()[dag_id] = create_ingest_dags(
-        source_name=source_name,
-        base_path_ingest=base_path_ingest,
-        conn_id_ingest=conn_id_ingest,
-        base_path_intermediate=base_path_intermediate,
-        conn_id_intermediate=conn_id_intermediate,
-        base_path_load=base_path_load,
-        conn_id_load=conn_id_load,
-        dataset_uri=dataset_uri,
-        dag_id=dag_id,
-        start_date=start_date,
-        schedule=schedule,
-        catchup=catchup,
-        tags=tags,
-        dag_owner=dag_owner,
-        task_retries=task_retries,
-    )
diff --git a/data-engineering-use-case/ingestion/load_to_snowflake.py b/data-engineering-use-case/ingestion/load_to_snowflake.py
deleted file mode 100644
index 10589cd..0000000
--- a/data-engineering-use-case/ingestion/load_to_snowflake.py
+++ /dev/null
@@ -1,100 +0,0 @@
-"""
-## Load to Snowflake
-
-This DAG loads data from the loading location in S3 to Snowflake.
-To use this DAG you need a stage for each source in Snowflake.
-
-For example for 3 sources you would need to create these 3 stages:
-
-```sql
-    CREATE STAGE sales_reports_stage
-    URL = 's3://ce-2-8-examples-bucket/load/sales_reports/'
-    CREDENTIALS = (AWS_KEY_ID = '<your aws key id>' AWS_SECRET_KEY = '<your aws secret>')
-    FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1);
-
-    CREATE STAGE customer_feedback_stage
-    URL = 's3://ce-2-8-examples-bucket/load/customer_feedback/'
-    CREDENTIALS = (AWS_KEY_ID = '<your aws key id>' AWS_SECRET_KEY = '<your aws secret>')
-    FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1);
-
-    CREATE STAGE customer_data_stage
-    URL = 's3://ce-2-8-examples-bucket/load/customer_data/'
-    CREDENTIALS = (AWS_KEY_ID = '<your aws key id>' AWS_SECRET_KEY = '<your aws secret>')
-    FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1);
-```
-
-"""
-
-from airflow.datasets import Dataset
-from airflow.decorators import dag
-from airflow.models.baseoperator import chain
-from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
-from pendulum import datetime
-import json
-from functools import reduce
-
-with open("include/ingestion_source_config.json", "r") as f:
-    config = json.load(f)
-
-    ingestion_datasets = [
-        Dataset(source["dataset_uri"]) for source in config["sources"]
-    ]
-
-    ingestion_datasets = tuple(ingestion_datasets)
-
-
-SNOWFLAKE_CONN_ID = "snowflake_de_team"
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule=reduce(
-        lambda x, y: x | y, ingestion_datasets
-    ),  # NEW in Airflow 2.9: Schedule on logical expressions involving datasets
-    catchup=False,
-    tags=["Conditional Dataset Scheduling", "use-case"],
-    default_args={"owner": "Piglet", "retries": 3, "retry_delay": 5},
-    description="Load data from S3 to Snowflake",
-    doc_md=__doc__,
-)
-def load_to_snowflake():
-    create_file_format = SnowflakeOperator(
-        task_id="create_file_format",
-        sql="""
-            CREATE FILE FORMAT IF NOT EXISTS my_csv_format 
-            TYPE = 'CSV'
-            FIELD_OPTIONALLY_ENCLOSED_BY = '"'
-            SKIP_HEADER = 1;
-        """,
-        snowflake_conn_id=SNOWFLAKE_CONN_ID,
-    )
-
-    for source_name, table_creation_sql in [
-        (source["source_name"], source["table_creation_sql"])
-        for source in config["sources"]
-    ]:
-        create_table_if_not_exists = SnowflakeOperator(
-            task_id=f"create_table_if_not_exists_{source_name}",
-            sql=table_creation_sql,
-            snowflake_conn_id=SNOWFLAKE_CONN_ID,
-        )
-
-        load_data = SnowflakeOperator(
-            task_id=f"load_{source_name}",
-            snowflake_conn_id=SNOWFLAKE_CONN_ID,
-            sql=f"""
-            COPY INTO {source_name}_table
-            FROM @{source_name}_stage
-            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1);
-            """,
-            outlets=[Dataset(f"snowflake://{source_name}_table")],
-        )
-
-        chain(
-            create_file_format,
-            create_table_if_not_exists,
-            load_data,
-        )
-
-
-load_to_snowflake()
diff --git a/data-engineering-use-case/prepare_earnings_report.py b/data-engineering-use-case/prepare_earnings_report.py
deleted file mode 100644
index 4665693..0000000
--- a/data-engineering-use-case/prepare_earnings_report.py
+++ /dev/null
@@ -1,118 +0,0 @@
-"""
-## Prepares the earnings report for a given toy
-
-The toy can be selected in manual runs as an Airflow param. 
-By default the report will be on our flagship product, the Carrot Plushy.
-"""
-
-from airflow.datasets import Dataset
-from airflow.decorators import dag, task
-from airflow.models.param import Param
-from airflow.models.baseoperator import chain
-from airflow.timetables.datasets import DatasetOrTimeSchedule
-from airflow.timetables.trigger import CronTriggerTimetable
-from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
-from pendulum import datetime
-from include.helpers import (
-    set_up_report_platform,
-    execute_report_mailer,
-    tear_down_report_platform,
-)
-
-SNOWFLAKE_CONN_ID = "snowflake_de_team"
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule=DatasetOrTimeSchedule(
-        timetable=CronTriggerTimetable("0 * * * *", timezone="UTC"),
-        datasets=[Dataset("snowflake://sales_reports_table")],
-    ),  # NEW in Airflow 2.9: Schedule on time and datasets
-    catchup=False,
-    tags=["DatasetOrTimeSchedule", "use-case"],
-    default_args={"owner": "Cerberus", "retries": 3, "retry_delay": 5},
-    description="Load data from S3 to Snowflake",
-    doc_md=__doc__,
-    params={
-        "toy_to_report": Param(
-            "Carrot Plushy",
-            type="string",
-            title="Core toy to report",
-            description="Define which toy to generate a report on.",
-            enum=[
-                "Carrot Plushy",
-                "ChewChew Train Dog Bed",
-                "Where is the Ball? - Transparent Edition",
-                "Stack of Artisinal Homework",
-                "Post breakfast treats - calory free",
-            ],
-        ),
-        "simulate_metric_fetch_failure": Param(
-            False,
-            type="boolean",
-            title="Simulate metric fetch failure",
-            description="Simulate a failure in the metric fetch set to True to see Setup/Teardown in Action.",
-        ),
-        "simulate_metric_mail_failure": Param(
-            False,
-            type="boolean",
-            title="Simulate metric mail failure",
-            description="Simulate a failure in the metric mailing set to True to see Setup/Teardown in Action.",
-        ),
-    },
-)
-def prepare_earnings_report():
-    @task
-    def set_up_internal_reporting_platform():
-        r = set_up_report_platform()
-        return r
-
-    @task(retries=0)
-    def get_key_metrics(**context):
-        key_metric = context["params"]["toy_to_report"]
-        simulate_metric_fetch_failure = context["params"][
-            "simulate_metric_fetch_failure"
-        ]
-        if simulate_metric_fetch_failure:
-            raise Exception("Metric fetch failed!")
-        return key_metric
-
-    get_total_earnings = SnowflakeOperator(
-        task_id=f"get_total_earnings",
-        snowflake_conn_id=SNOWFLAKE_CONN_ID,
-        sql="""
-        SELECT SUM(REVENUE) FROM sales_reports_table 
-        WHERE PRODUCTNAME = '{{ ti.xcom_pull(task_ids='get_key_metrics') }}';
-        """,
-    )
-
-    @task(retries=0)
-    def send_mails_internal_platform(**context):
-        simulate_failure = context["params"]["simulate_metric_mail_failure"]
-        if simulate_failure:
-            raise Exception("Metric mailing failed!")
-        r = execute_report_mailer()
-        return r
-
-    @task
-    def tear_down_internal_reporting_platform():
-        r = tear_down_report_platform()
-        return r
-
-    set_up_internal_reporting_platform_obj = set_up_internal_reporting_platform()
-    tear_down_internal_reporting_platform_obj = tear_down_internal_reporting_platform()
-
-    tear_down_internal_reporting_platform_obj.as_teardown(
-        setups=[set_up_internal_reporting_platform_obj]
-    )
-
-    chain(
-        set_up_internal_reporting_platform_obj,
-        get_key_metrics(),
-        get_total_earnings,
-        send_mails_internal_platform(),
-        tear_down_internal_reporting_platform_obj,
-    )
-
-
-prepare_earnings_report()
diff --git a/example_dags/__init__.py b/example_dags/__init__.py
deleted file mode 100644
index 217e5db..0000000
--- a/example_dags/__init__.py
+++ /dev/null
@@ -1,17 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
diff --git a/example_dags/example_bash_decorator.py b/example_dags/example_bash_decorator.py
deleted file mode 100644
index 36c9c2d..0000000
--- a/example_dags/example_bash_decorator.py
+++ /dev/null
@@ -1,116 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import dag, task
-from airflow.exceptions import AirflowSkipException
-from airflow.models.baseoperator import chain
-from airflow.operators.empty import EmptyOperator
-from airflow.utils.trigger_rule import TriggerRule
-from airflow.utils.weekday import WeekDay
-
-
-@dag(schedule=None, start_date=pendulum.datetime(2023, 1, 1, tz="UTC"), catchup=False)
-def example_bash_decorator():
-    @task.bash
-    def run_me(sleep_seconds: int, task_instance_key_str: str) -> str:
-        return f"echo {task_instance_key_str} && sleep {sleep_seconds}"
-
-    run_me_loop = [run_me.override(task_id=f"runme_{i}")(sleep_seconds=i) for i in range(3)]
-
-    # [START howto_decorator_bash]
-    @task.bash
-    def run_after_loop() -> str:
-        return "echo https://airflow.apache.org/"
-
-    run_this = run_after_loop()
-    # [END howto_decorator_bash]
-
-    # [START howto_decorator_bash_template]
-    @task.bash
-    def also_run_this() -> str:
-        return 'echo "ti_key={{ task_instance_key_str }}"'
-
-    also_this = also_run_this()
-    # [END howto_decorator_bash_template]
-
-    # [START howto_decorator_bash_context_vars]
-    @task.bash
-    def also_run_this_again(task_instance_key_str) -> str:
-        return f'echo "ti_key={task_instance_key_str}"'
-
-    also_this_again = also_run_this_again()
-    # [END howto_decorator_bash_context_vars]
-
-    # [START howto_decorator_bash_skip]
-    @task.bash
-    def this_will_skip() -> str:
-        return 'echo "hello world"; exit 99;'
-
-    this_skips = this_will_skip()
-    # [END howto_decorator_bash_skip]
-
-    run_this_last = EmptyOperator(task_id="run_this_last", trigger_rule=TriggerRule.ALL_DONE)
-
-    # [START howto_decorator_bash_conditional]
-    @task.bash
-    def sleep_in(day: str) -> str:
-        if day in (WeekDay.SATURDAY, WeekDay.SUNDAY):
-            return f"sleep {60 * 60}"
-        else:
-            raise AirflowSkipException("No sleeping in today!")
-
-    sleep_in(day="{{ dag_run.logical_date.strftime('%A').lower() }}")
-    # [END howto_decorator_bash_conditional]
-
-    # [START howto_decorator_bash_parametrize]
-    @task.bash(env={"BASE_DIR": "{{ dag_run.logical_date.strftime('%Y/%m/%d') }}"}, append_env=True)
-    def make_dynamic_dirs(new_dirs: str) -> str:
-        return f"mkdir -p $AIRFLOW_HOME/$BASE_DIR/{new_dirs}"
-
-    make_dynamic_dirs(new_dirs="foo/bar/baz")
-    # [END howto_decorator_bash_parametrize]
-
-    # [START howto_decorator_bash_build_cmd]
-    def _get_files_in_cwd() -> list[str]:
-        from pathlib import Path
-
-        dir_contents = Path.cwd().glob("airflow/example_dags/*.py")
-        files = [str(elem) for elem in dir_contents if elem.is_file()]
-
-        return files
-
-    @task.bash
-    def get_file_stats() -> str:
-        from shlex import join
-
-        files = _get_files_in_cwd()
-        cmd = join(["stat", *files])
-
-        return cmd
-
-    get_file_stats()
-    # [END howto_decorator_bash_build_cmd]
-
-    chain(run_me_loop, run_this)
-    chain([also_this, also_this_again, this_skips, run_this], run_this_last)
-
-
-example_bash_decorator()
diff --git a/example_dags/example_bash_operator.py b/example_dags/example_bash_operator.py
deleted file mode 100644
index b08d31c..0000000
--- a/example_dags/example_bash_operator.py
+++ /dev/null
@@ -1,77 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the BashOperator."""
-
-from __future__ import annotations
-
-import datetime
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-from airflow.operators.empty import EmptyOperator
-
-with DAG(
-    dag_id="example_bash_operator",
-    schedule="0 0 * * *",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    dagrun_timeout=datetime.timedelta(minutes=60),
-    tags=["example", "example2"],
-    params={"example_key": "example_value"},
-) as dag:
-    run_this_last = EmptyOperator(
-        task_id="run_this_last",
-    )
-
-    # [START howto_operator_bash]
-    run_this = BashOperator(
-        task_id="run_after_loop",
-        bash_command="echo https://airflow.apache.org/",
-    )
-    # [END howto_operator_bash]
-
-    run_this >> run_this_last
-
-    for i in range(3):
-        task = BashOperator(
-            task_id=f"runme_{i}",
-            bash_command='echo "{{ task_instance_key_str }}" && sleep 1',
-        )
-        task >> run_this
-
-    # [START howto_operator_bash_template]
-    also_run_this = BashOperator(
-        task_id="also_run_this",
-        bash_command='echo "ti_key={{ task_instance_key_str }}"',
-    )
-    # [END howto_operator_bash_template]
-    also_run_this >> run_this_last
-
-# [START howto_operator_bash_skip]
-this_will_skip = BashOperator(
-    task_id="this_will_skip",
-    bash_command='echo "hello world"; exit 99;',
-    dag=dag,
-)
-# [END howto_operator_bash_skip]
-this_will_skip >> run_this_last
-
-if __name__ == "__main__":
-    dag.test()
diff --git a/example_dags/example_branch_datetime_operator.py b/example_dags/example_branch_datetime_operator.py
deleted file mode 100644
index 99645b9..0000000
--- a/example_dags/example_branch_datetime_operator.py
+++ /dev/null
@@ -1,105 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of DateTimeBranchOperator with datetime as well as time objects as
-targets.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.providers.standard.operators.datetime import BranchDateTimeOperator
-
-dag1 = DAG(
-    dag_id="example_branch_datetime_operator",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-    schedule="@daily",
-)
-
-# [START howto_branch_datetime_operator]
-empty_task_11 = EmptyOperator(task_id="date_in_range", dag=dag1)
-empty_task_21 = EmptyOperator(task_id="date_outside_range", dag=dag1)
-
-cond1 = BranchDateTimeOperator(
-    task_id="datetime_branch",
-    follow_task_ids_if_true=["date_in_range"],
-    follow_task_ids_if_false=["date_outside_range"],
-    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),
-    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),
-    dag=dag1,
-)
-
-# Run empty_task_11 if cond1 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00
-cond1 >> [empty_task_11, empty_task_21]
-# [END howto_branch_datetime_operator]
-
-
-dag2 = DAG(
-    dag_id="example_branch_datetime_operator_2",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-    schedule="@daily",
-)
-# [START howto_branch_datetime_operator_next_day]
-empty_task_12 = EmptyOperator(task_id="date_in_range", dag=dag2)
-empty_task_22 = EmptyOperator(task_id="date_outside_range", dag=dag2)
-
-cond2 = BranchDateTimeOperator(
-    task_id="datetime_branch",
-    follow_task_ids_if_true=["date_in_range"],
-    follow_task_ids_if_false=["date_outside_range"],
-    target_upper=pendulum.time(0, 0, 0),
-    target_lower=pendulum.time(15, 0, 0),
-    dag=dag2,
-)
-
-# Since target_lower happens after target_upper, target_upper will be moved to the following day
-# Run empty_task_12 if cond2 executes between 15:00:00, and 00:00:00 of the following day
-cond2 >> [empty_task_12, empty_task_22]
-# [END howto_branch_datetime_operator_next_day]
-
-dag3 = DAG(
-    dag_id="example_branch_datetime_operator_3",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-    schedule="@daily",
-)
-# [START howto_branch_datetime_operator_logical_date]
-empty_task_13 = EmptyOperator(task_id="date_in_range", dag=dag3)
-empty_task_23 = EmptyOperator(task_id="date_outside_range", dag=dag3)
-
-cond3 = BranchDateTimeOperator(
-    task_id="datetime_branch",
-    use_task_logical_date=True,
-    follow_task_ids_if_true=["date_in_range"],
-    follow_task_ids_if_false=["date_outside_range"],
-    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),
-    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),
-    dag=dag3,
-)
-
-# Run empty_task_13 if cond3 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00
-cond3 >> [empty_task_13, empty_task_23]
-# [END howto_branch_datetime_operator_logical_date]
diff --git a/example_dags/example_branch_day_of_week_operator.py b/example_dags/example_branch_day_of_week_operator.py
deleted file mode 100644
index b7eed08..0000000
--- a/example_dags/example_branch_day_of_week_operator.py
+++ /dev/null
@@ -1,61 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of BranchDayOfWeekOperator.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.providers.standard.operators.weekday import BranchDayOfWeekOperator
-from airflow.utils.weekday import WeekDay
-
-with DAG(
-    dag_id="example_weekday_branch_operator",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-    schedule="@daily",
-) as dag:
-    # [START howto_operator_day_of_week_branch]
-    empty_task_1 = EmptyOperator(task_id="branch_true")
-    empty_task_2 = EmptyOperator(task_id="branch_false")
-    empty_task_3 = EmptyOperator(task_id="branch_weekend")
-    empty_task_4 = EmptyOperator(task_id="branch_mid_week")
-
-    branch = BranchDayOfWeekOperator(
-        task_id="make_choice",
-        follow_task_ids_if_true="branch_true",
-        follow_task_ids_if_false="branch_false",
-        week_day="Monday",
-    )
-    branch_weekend = BranchDayOfWeekOperator(
-        task_id="make_weekend_choice",
-        follow_task_ids_if_true="branch_weekend",
-        follow_task_ids_if_false="branch_mid_week",
-        week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},
-    )
-
-    # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise
-    branch >> [empty_task_1, empty_task_2]
-    # Run empty_task_3 if it's a weekend, empty_task_4 otherwise
-    empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]
-    # [END howto_operator_day_of_week_branch]
diff --git a/example_dags/example_branch_labels.py b/example_dags/example_branch_labels.py
deleted file mode 100644
index 7e90f26..0000000
--- a/example_dags/example_branch_labels.py
+++ /dev/null
@@ -1,46 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of labels with different branches.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.utils.edgemodifier import Label
-
-with DAG(
-    "example_branch_labels",
-    schedule="@daily",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-) as dag:
-    ingest = EmptyOperator(task_id="ingest")
-    analyse = EmptyOperator(task_id="analyze")
-    check = EmptyOperator(task_id="check_integrity")
-    describe = EmptyOperator(task_id="describe_integrity")
-    error = EmptyOperator(task_id="email_error")
-    save = EmptyOperator(task_id="save")
-    report = EmptyOperator(task_id="report")
-
-    ingest >> analyse >> check
-    check >> Label("No errors") >> save >> report
-    check >> Label("Errors found") >> describe >> error >> report
diff --git a/example_dags/example_branch_operator.py b/example_dags/example_branch_operator.py
deleted file mode 100644
index 492d631..0000000
--- a/example_dags/example_branch_operator.py
+++ /dev/null
@@ -1,171 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the Classic branching Python operators.
-
-It is showcasing the basic BranchPythonOperator and its sisters BranchExternalPythonOperator
-and BranchPythonVirtualenvOperator."""
-
-from __future__ import annotations
-
-import random
-import sys
-import tempfile
-from pathlib import Path
-
-import pendulum
-
-from airflow.operators.python import is_venv_installed
-
-if is_venv_installed():
-    from airflow.models.dag import DAG
-    from airflow.operators.empty import EmptyOperator
-    from airflow.operators.python import (
-        BranchExternalPythonOperator,
-        BranchPythonOperator,
-        BranchPythonVirtualenvOperator,
-        ExternalPythonOperator,
-        PythonOperator,
-        PythonVirtualenvOperator,
-    )
-    from airflow.utils.edgemodifier import Label
-    from airflow.utils.trigger_rule import TriggerRule
-
-    PATH_TO_PYTHON_BINARY = sys.executable
-
-    with DAG(
-        dag_id="example_branch_operator",
-        start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-        catchup=False,
-        schedule="@daily",
-        tags=["example", "example2"],
-        orientation="TB",
-    ) as dag:
-        run_this_first = EmptyOperator(
-            task_id="run_this_first",
-        )
-
-        options = ["a", "b", "c", "d"]
-
-        # Example branching on standard Python tasks
-
-        # [START howto_operator_branch_python]
-        branching = BranchPythonOperator(
-            task_id="branching",
-            python_callable=lambda: f"branch_{random.choice(options)}",
-        )
-        # [END howto_operator_branch_python]
-        run_this_first >> branching
-
-        join = EmptyOperator(
-            task_id="join",
-            trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
-        )
-
-        for option in options:
-            t = PythonOperator(
-                task_id=f"branch_{option}",
-                python_callable=lambda: print("Hello World"),
-            )
-
-            empty_follow = EmptyOperator(
-                task_id="follow_" + option,
-            )
-
-            # Label is optional here, but it can help identify more complex branches
-            branching >> Label(option) >> t >> empty_follow >> join
-
-        # Example the same with external Python calls
-
-        # [START howto_operator_branch_ext_py]
-        def branch_with_external_python(choices):
-            import random
-
-            return f"ext_py_{random.choice(choices)}"
-
-        branching_ext_py = BranchExternalPythonOperator(
-            task_id="branching_ext_python",
-            python=PATH_TO_PYTHON_BINARY,
-            python_callable=branch_with_external_python,
-            op_args=[options],
-        )
-        # [END howto_operator_branch_ext_py]
-        join >> branching_ext_py
-
-        join_ext_py = EmptyOperator(
-            task_id="join_ext_python",
-            trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
-        )
-
-        def hello_world_with_external_python():
-            print("Hello World from external Python")
-
-        for option in options:
-            t = ExternalPythonOperator(
-                task_id=f"ext_py_{option}",
-                python=PATH_TO_PYTHON_BINARY,
-                python_callable=hello_world_with_external_python,
-            )
-
-            # Label is optional here, but it can help identify more complex branches
-            branching_ext_py >> Label(option) >> t >> join_ext_py
-
-        # Example the same with Python virtual environments
-
-        # [START howto_operator_branch_virtualenv]
-        # Note: Passing a caching dir allows to keep the virtual environment over multiple runs
-        #       Run the example a second time and see that it re-uses it and is faster.
-        VENV_CACHE_PATH = Path(tempfile.gettempdir())
-
-        def branch_with_venv(choices):
-            import random
-
-            import numpy as np
-
-            print(f"Some numpy stuff: {np.arange(6)}")
-            return f"venv_{random.choice(choices)}"
-
-        branching_venv = BranchPythonVirtualenvOperator(
-            task_id="branching_venv",
-            requirements=["numpy~=1.26.0"],
-            venv_cache_path=VENV_CACHE_PATH,
-            python_callable=branch_with_venv,
-            op_args=[options],
-        )
-        # [END howto_operator_branch_virtualenv]
-        join_ext_py >> branching_venv
-
-        join_venv = EmptyOperator(
-            task_id="join_venv",
-            trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
-        )
-
-        def hello_world_with_venv():
-            import numpy as np
-
-            print(f"Hello World with some numpy stuff: {np.arange(6)}")
-
-        for option in options:
-            t = PythonVirtualenvOperator(
-                task_id=f"venv_{option}",
-                requirements=["numpy~=1.26.0"],
-                venv_cache_path=VENV_CACHE_PATH,
-                python_callable=hello_world_with_venv,
-            )
-
-            # Label is optional here, but it can help identify more complex branches
-            branching_venv >> Label(option) >> t >> join_venv
diff --git a/example_dags/example_branch_operator_decorator.py b/example_dags/example_branch_operator_decorator.py
deleted file mode 100644
index 59cb3b2..0000000
--- a/example_dags/example_branch_operator_decorator.py
+++ /dev/null
@@ -1,150 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the branching TaskFlow API decorators.
-
-It shows how to use standard Python ``@task.branch`` as well as the external Python
-version ``@task.branch_external_python`` which calls an external Python interpreter and
-the ``@task.branch_virtualenv`` which builds a temporary Python virtual environment.
-"""
-
-from __future__ import annotations
-
-import random
-import sys
-import tempfile
-
-import pendulum
-
-from airflow.operators.python import is_venv_installed
-
-if is_venv_installed():
-    from airflow.decorators import task
-    from airflow.models.dag import DAG
-    from airflow.operators.empty import EmptyOperator
-    from airflow.utils.edgemodifier import Label
-    from airflow.utils.trigger_rule import TriggerRule
-
-    PATH_TO_PYTHON_BINARY = sys.executable
-
-    with DAG(
-        dag_id="example_branch_python_operator_decorator",
-        start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-        catchup=False,
-        schedule="@daily",
-        tags=["example", "example2"],
-        orientation="TB",
-    ) as dag:
-        run_this_first = EmptyOperator(task_id="run_this_first")
-
-        options = ["a", "b", "c", "d"]
-
-        # Example branching on standard Python tasks
-
-        # [START howto_operator_branch_python]
-        @task.branch()
-        def branching(choices: list[str]) -> str:
-            return f"branch_{random.choice(choices)}"
-
-        # [END howto_operator_branch_python]
-
-        random_choice_instance = branching(choices=options)
-
-        run_this_first >> random_choice_instance
-
-        join = EmptyOperator(task_id="join", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)
-
-        for option in options:
-
-            @task(task_id=f"branch_{option}")
-            def some_task():
-                print("doing something in Python")
-
-            t = some_task()
-            empty = EmptyOperator(task_id=f"follow_{option}")
-
-            # Label is optional here, but it can help identify more complex branches
-            random_choice_instance >> Label(option) >> t >> empty >> join
-
-        # Example the same with external Python calls
-
-        # [START howto_operator_branch_ext_py]
-        @task.branch_external_python(python=PATH_TO_PYTHON_BINARY)
-        def branching_ext_python(choices) -> str:
-            import random
-
-            return f"ext_py_{random.choice(choices)}"
-
-        # [END howto_operator_branch_ext_py]
-
-        random_choice_ext_py = branching_ext_python(choices=options)
-
-        join >> random_choice_ext_py
-
-        join_ext_py = EmptyOperator(
-            task_id="join_ext_py", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS
-        )
-
-        for option in options:
-
-            @task.external_python(task_id=f"ext_py_{option}", python=PATH_TO_PYTHON_BINARY)
-            def some_ext_py_task():
-                print("doing something in external Python")
-
-            t = some_ext_py_task()
-
-            # Label is optional here, but it can help identify more complex branches
-            random_choice_ext_py >> Label(option) >> t >> join_ext_py
-
-        # Example the same with Python virtual environments
-
-        # [START howto_operator_branch_virtualenv]
-        # Note: Passing a caching dir allows to keep the virtual environment over multiple runs
-        #       Run the example a second time and see that it re-uses it and is faster.
-        VENV_CACHE_PATH = tempfile.gettempdir()
-
-        @task.branch_virtualenv(requirements=["numpy~=1.24.4"], venv_cache_path=VENV_CACHE_PATH)
-        def branching_virtualenv(choices) -> str:
-            import random
-
-            import numpy as np
-
-            print(f"Some numpy stuff: {np.arange(6)}")
-            return f"venv_{random.choice(choices)}"
-
-        # [END howto_operator_branch_virtualenv]
-
-        random_choice_venv = branching_virtualenv(choices=options)
-
-        join_ext_py >> random_choice_venv
-
-        join_venv = EmptyOperator(task_id="join_venv", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)
-
-        for option in options:
-
-            @task.virtualenv(
-                task_id=f"venv_{option}", requirements=["numpy~=1.24.4"], venv_cache_path=VENV_CACHE_PATH
-            )
-            def some_venv_task():
-                import numpy as np
-
-                print(f"Some numpy stuff: {np.arange(6)}")
-
-            t = some_venv_task()
-
-            # Label is optional here, but it can help identify more complex branches
-            random_choice_venv >> Label(option) >> t >> join_venv
diff --git a/example_dags/example_branch_python_dop_operator_3.py b/example_dags/example_branch_python_dop_operator_3.py
deleted file mode 100644
index a703196..0000000
--- a/example_dags/example_branch_python_dop_operator_3.py
+++ /dev/null
@@ -1,61 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of ``@task.branch`` TaskFlow API decorator with depends_on_past=True,
-where tasks may be run or skipped on alternating runs.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-
-
-@task.branch()
-def should_run(**kwargs) -> str:
-    """
-    Determine which empty_task should be run based on if the execution date minute is even or odd.
-
-    :param dict kwargs: Context
-    :return: Id of the task to run
-    """
-    print(
-        f"------------- exec dttm = {kwargs['execution_date']} and minute = {kwargs['execution_date'].minute}"
-    )
-    if kwargs["execution_date"].minute % 2 == 0:
-        return "empty_task_1"
-    else:
-        return "empty_task_2"
-
-
-with DAG(
-    dag_id="example_branch_dop_operator_v3",
-    schedule="*/1 * * * *",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    default_args={"depends_on_past": True},
-    tags=["example"],
-) as dag:
-    cond = should_run()
-
-    empty_task_1 = EmptyOperator(task_id="empty_task_1")
-    empty_task_2 = EmptyOperator(task_id="empty_task_2")
-    cond >> [empty_task_1, empty_task_2]
diff --git a/example_dags/example_complex.py b/example_dags/example_complex.py
deleted file mode 100644
index e7eba78..0000000
--- a/example_dags/example_complex.py
+++ /dev/null
@@ -1,221 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example Airflow DAG that shows the complex DAG structure.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.baseoperator import chain
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-
-with DAG(
-    dag_id="example_complex",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example", "example2", "example3"],
-) as dag:
-    # Create
-    create_entry_group = BashOperator(task_id="create_entry_group", bash_command="echo create_entry_group")
-
-    create_entry_group_result = BashOperator(
-        task_id="create_entry_group_result", bash_command="echo create_entry_group_result"
-    )
-
-    create_entry_group_result2 = BashOperator(
-        task_id="create_entry_group_result2", bash_command="echo create_entry_group_result2"
-    )
-
-    create_entry_gcs = BashOperator(task_id="create_entry_gcs", bash_command="echo create_entry_gcs")
-
-    create_entry_gcs_result = BashOperator(
-        task_id="create_entry_gcs_result", bash_command="echo create_entry_gcs_result"
-    )
-
-    create_entry_gcs_result2 = BashOperator(
-        task_id="create_entry_gcs_result2", bash_command="echo create_entry_gcs_result2"
-    )
-
-    create_tag = BashOperator(task_id="create_tag", bash_command="echo create_tag")
-
-    create_tag_result = BashOperator(task_id="create_tag_result", bash_command="echo create_tag_result")
-
-    create_tag_result2 = BashOperator(task_id="create_tag_result2", bash_command="echo create_tag_result2")
-
-    create_tag_template = BashOperator(task_id="create_tag_template", bash_command="echo create_tag_template")
-
-    create_tag_template_result = BashOperator(
-        task_id="create_tag_template_result", bash_command="echo create_tag_template_result"
-    )
-
-    create_tag_template_result2 = BashOperator(
-        task_id="create_tag_template_result2", bash_command="echo create_tag_template_result2"
-    )
-
-    create_tag_template_field = BashOperator(
-        task_id="create_tag_template_field", bash_command="echo create_tag_template_field"
-    )
-
-    create_tag_template_field_result = BashOperator(
-        task_id="create_tag_template_field_result", bash_command="echo create_tag_template_field_result"
-    )
-
-    create_tag_template_field_result2 = BashOperator(
-        task_id="create_tag_template_field_result2", bash_command="echo create_tag_template_field_result"
-    )
-
-    # Delete
-    delete_entry = BashOperator(task_id="delete_entry", bash_command="echo delete_entry")
-    create_entry_gcs >> delete_entry
-
-    delete_entry_group = BashOperator(task_id="delete_entry_group", bash_command="echo delete_entry_group")
-    create_entry_group >> delete_entry_group
-
-    delete_tag = BashOperator(task_id="delete_tag", bash_command="echo delete_tag")
-    create_tag >> delete_tag
-
-    delete_tag_template_field = BashOperator(
-        task_id="delete_tag_template_field", bash_command="echo delete_tag_template_field"
-    )
-
-    delete_tag_template = BashOperator(task_id="delete_tag_template", bash_command="echo delete_tag_template")
-
-    # Get
-    get_entry_group = BashOperator(task_id="get_entry_group", bash_command="echo get_entry_group")
-
-    get_entry_group_result = BashOperator(
-        task_id="get_entry_group_result", bash_command="echo get_entry_group_result"
-    )
-
-    get_entry = BashOperator(task_id="get_entry", bash_command="echo get_entry")
-
-    get_entry_result = BashOperator(task_id="get_entry_result", bash_command="echo get_entry_result")
-
-    get_tag_template = BashOperator(task_id="get_tag_template", bash_command="echo get_tag_template")
-
-    get_tag_template_result = BashOperator(
-        task_id="get_tag_template_result", bash_command="echo get_tag_template_result"
-    )
-
-    # List
-    list_tags = BashOperator(task_id="list_tags", bash_command="echo list_tags")
-
-    list_tags_result = BashOperator(task_id="list_tags_result", bash_command="echo list_tags_result")
-
-    # Lookup
-    lookup_entry = BashOperator(task_id="lookup_entry", bash_command="echo lookup_entry")
-
-    lookup_entry_result = BashOperator(task_id="lookup_entry_result", bash_command="echo lookup_entry_result")
-
-    # Rename
-    rename_tag_template_field = BashOperator(
-        task_id="rename_tag_template_field", bash_command="echo rename_tag_template_field"
-    )
-
-    # Search
-    search_catalog = BashOperator(task_id="search_catalog", bash_command="echo search_catalog")
-
-    search_catalog_result = BashOperator(
-        task_id="search_catalog_result", bash_command="echo search_catalog_result"
-    )
-
-    # Update
-    update_entry = BashOperator(task_id="update_entry", bash_command="echo update_entry")
-
-    update_tag = BashOperator(task_id="update_tag", bash_command="echo update_tag")
-
-    update_tag_template = BashOperator(task_id="update_tag_template", bash_command="echo update_tag_template")
-
-    update_tag_template_field = BashOperator(
-        task_id="update_tag_template_field", bash_command="echo update_tag_template_field"
-    )
-
-    # Create
-    create_tasks = [
-        create_entry_group,
-        create_entry_gcs,
-        create_tag_template,
-        create_tag_template_field,
-        create_tag,
-    ]
-    chain(*create_tasks)
-
-    create_entry_group >> delete_entry_group
-    create_entry_group >> create_entry_group_result
-    create_entry_group >> create_entry_group_result2
-
-    create_entry_gcs >> delete_entry
-    create_entry_gcs >> create_entry_gcs_result
-    create_entry_gcs >> create_entry_gcs_result2
-
-    create_tag_template >> delete_tag_template_field
-    create_tag_template >> create_tag_template_result
-    create_tag_template >> create_tag_template_result2
-
-    create_tag_template_field >> delete_tag_template_field
-    create_tag_template_field >> create_tag_template_field_result
-    create_tag_template_field >> create_tag_template_field_result2
-
-    create_tag >> delete_tag
-    create_tag >> create_tag_result
-    create_tag >> create_tag_result2
-
-    # Delete
-    delete_tasks = [
-        delete_tag,
-        delete_tag_template_field,
-        delete_tag_template,
-        delete_entry_group,
-        delete_entry,
-    ]
-    chain(*delete_tasks)
-
-    # Get
-    create_tag_template >> get_tag_template >> delete_tag_template
-    get_tag_template >> get_tag_template_result
-
-    create_entry_gcs >> get_entry >> delete_entry
-    get_entry >> get_entry_result
-
-    create_entry_group >> get_entry_group >> delete_entry_group
-    get_entry_group >> get_entry_group_result
-
-    # List
-    create_tag >> list_tags >> delete_tag
-    list_tags >> list_tags_result
-
-    # Lookup
-    create_entry_gcs >> lookup_entry >> delete_entry
-    lookup_entry >> lookup_entry_result
-
-    # Rename
-    create_tag_template_field >> rename_tag_template_field >> delete_tag_template_field
-
-    # Search
-    chain(create_tasks, search_catalog, delete_tasks)
-    search_catalog >> search_catalog_result
-
-    # Update
-    create_entry_gcs >> update_entry >> delete_entry
-    create_tag >> update_tag >> delete_tag
-    create_tag_template >> update_tag_template >> delete_tag_template
-    create_tag_template_field >> update_tag_template_field >> rename_tag_template_field
diff --git a/example_dags/example_dag_decorator.py b/example_dags/example_dag_decorator.py
deleted file mode 100644
index 447b447..0000000
--- a/example_dags/example_dag_decorator.py
+++ /dev/null
@@ -1,75 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-from typing import TYPE_CHECKING, Any
-
-import httpx
-import pendulum
-
-from airflow.decorators import dag, task
-from airflow.models.baseoperator import BaseOperator
-from airflow.operators.email import EmailOperator
-
-if TYPE_CHECKING:
-    from airflow.utils.context import Context
-
-
-class GetRequestOperator(BaseOperator):
-    """Custom operator to send GET request to provided url"""
-
-    def __init__(self, *, url: str, **kwargs):
-        super().__init__(**kwargs)
-        self.url = url
-
-    def execute(self, context: Context):
-        return httpx.get(self.url).json()
-
-
-# [START dag_decorator_usage]
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-)
-def example_dag_decorator(email: str = "example@example.com"):
-    """
-    DAG to send server IP to email.
-
-    :param email: Email to send IP to. Defaults to example@example.com.
-    """
-    get_ip = GetRequestOperator(task_id="get_ip", url="http://httpbin.org/get")
-
-    @task(multiple_outputs=True)
-    def prepare_email(raw_json: dict[str, Any]) -> dict[str, str]:
-        external_ip = raw_json["origin"]
-        return {
-            "subject": f"Server connected from {external_ip}",
-            "body": f"Seems like today your server executing Airflow is connected from IP {external_ip}<br>",
-        }
-
-    email_info = prepare_email(get_ip.output)
-
-    EmailOperator(
-        task_id="send_email", to=email, subject=email_info["subject"], html_content=email_info["body"]
-    )
-
-
-example_dag = example_dag_decorator()
-# [END dag_decorator_usage]
diff --git a/example_dags/example_dataset_alias.py b/example_dags/example_dataset_alias.py
deleted file mode 100644
index c50a89e..0000000
--- a/example_dags/example_dataset_alias.py
+++ /dev/null
@@ -1,101 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG for demonstrating the behavior of the DatasetAlias feature in Airflow, including conditional and
-dataset expression-based scheduling.
-
-Notes on usage:
-
-Turn on all the DAGs.
-
-Before running any DAG, the schedule of the "dataset_alias_example_alias_consumer" DAG will show as "Unresolved DatasetAlias".
-This is expected because the dataset alias has not been resolved into any dataset yet.
-
-Once the "dataset_s3_bucket_producer" DAG is triggered, the "dataset_s3_bucket_consumer" DAG should be triggered upon completion.
-This is because the dataset alias "example-alias" is used to add a dataset event to the dataset "s3://bucket/my-task"
-during the "produce_dataset_events_through_dataset_alias" task.
-As the DAG "dataset-alias-consumer" relies on dataset alias "example-alias" which was previously unresolved,
-the DAG "dataset-alias-consumer" (along with all the DAGs in the same file) will be re-parsed and
-thus update its schedule to the dataset "s3://bucket/my-task" and will also be triggered.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow import DAG
-from airflow.datasets import Dataset, DatasetAlias
-from airflow.decorators import task
-
-with DAG(
-    dag_id="dataset_s3_bucket_producer",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=None,
-    catchup=False,
-    tags=["producer", "dataset"],
-):
-
-    @task(outlets=[Dataset("s3://bucket/my-task")])
-    def produce_dataset_events():
-        pass
-
-    produce_dataset_events()
-
-with DAG(
-    dag_id="dataset_alias_example_alias_producer",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=None,
-    catchup=False,
-    tags=["producer", "dataset-alias"],
-):
-
-    @task(outlets=[DatasetAlias("example-alias")])
-    def produce_dataset_events_through_dataset_alias(*, outlet_events=None):
-        bucket_name = "bucket"
-        object_path = "my-task"
-        outlet_events["example-alias"].add(Dataset(f"s3://{bucket_name}/{object_path}"))
-
-    produce_dataset_events_through_dataset_alias()
-
-with DAG(
-    dag_id="dataset_s3_bucket_consumer",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[Dataset("s3://bucket/my-task")],
-    catchup=False,
-    tags=["consumer", "dataset"],
-):
-
-    @task
-    def consume_dataset_event():
-        pass
-
-    consume_dataset_event()
-
-with DAG(
-    dag_id="dataset_alias_example_alias_consumer",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[DatasetAlias("example-alias")],
-    catchup=False,
-    tags=["consumer", "dataset-alias"],
-):
-
-    @task(inlets=[DatasetAlias("example-alias")])
-    def consume_dataset_event_from_dataset_alias(*, inlet_events=None):
-        for event in inlet_events[DatasetAlias("example-alias")]:
-            print(event)
-
-    consume_dataset_event_from_dataset_alias()
diff --git a/example_dags/example_dataset_alias_with_no_taskflow.py b/example_dags/example_dataset_alias_with_no_taskflow.py
deleted file mode 100644
index 7d7227a..0000000
--- a/example_dags/example_dataset_alias_with_no_taskflow.py
+++ /dev/null
@@ -1,108 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG for demonstrating the behavior of the DatasetAlias feature in Airflow, including conditional and
-dataset expression-based scheduling.
-
-Notes on usage:
-
-Turn on all the DAGs.
-
-Before running any DAG, the schedule of the "dataset_alias_example_alias_consumer_with_no_taskflow" DAG will show as "unresolved DatasetAlias".
-This is expected because the dataset alias has not been resolved into any dataset yet.
-
-Once the "dataset_s3_bucket_producer_with_no_taskflow" DAG is triggered, the "dataset_s3_bucket_consumer_with_no_taskflow" DAG should be triggered upon completion.
-This is because the dataset alias "example-alias-no-taskflow" is used to add a dataset event to the dataset "s3://bucket/my-task-with-no-taskflow"
-during the "produce_dataset_events_through_dataset_alias_with_no_taskflow" task. Also, the schedule of the "dataset_alias_example_alias_consumer_with_no_taskflow" DAG should change to "Dataset" as
-the dataset alias "example-alias-no-taskflow" is now resolved to the dataset "s3://bucket/my-task-with-no-taskflow" and this DAG should also be triggered.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow import DAG
-from airflow.datasets import Dataset, DatasetAlias
-from airflow.operators.python import PythonOperator
-
-with DAG(
-    dag_id="dataset_s3_bucket_producer_with_no_taskflow",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=None,
-    catchup=False,
-    tags=["producer", "dataset"],
-):
-
-    def produce_dataset_events():
-        pass
-
-    PythonOperator(
-        task_id="produce_dataset_events",
-        outlets=[Dataset("s3://bucket/my-task-with-no-taskflow")],
-        python_callable=produce_dataset_events,
-    )
-
-
-with DAG(
-    dag_id="dataset_alias_example_alias_producer_with_no_taskflow",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=None,
-    catchup=False,
-    tags=["producer", "dataset-alias"],
-):
-
-    def produce_dataset_events_through_dataset_alias_with_no_taskflow(*, outlet_events=None):
-        bucket_name = "bucket"
-        object_path = "my-task"
-        outlet_events["example-alias-no-taskflow"].add(Dataset(f"s3://{bucket_name}/{object_path}"))
-
-    PythonOperator(
-        task_id="produce_dataset_events_through_dataset_alias_with_no_taskflow",
-        outlets=[DatasetAlias("example-alias-no-taskflow")],
-        python_callable=produce_dataset_events_through_dataset_alias_with_no_taskflow,
-    )
-
-with DAG(
-    dag_id="dataset_s3_bucket_consumer_with_no_taskflow",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[Dataset("s3://bucket/my-task-with-no-taskflow")],
-    catchup=False,
-    tags=["consumer", "dataset"],
-):
-
-    def consume_dataset_event():
-        pass
-
-    PythonOperator(task_id="consume_dataset_event", python_callable=consume_dataset_event)
-
-with DAG(
-    dag_id="dataset_alias_example_alias_consumer_with_no_taskflow",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[DatasetAlias("example-alias-no-taskflow")],
-    catchup=False,
-    tags=["consumer", "dataset-alias"],
-):
-
-    def consume_dataset_event_from_dataset_alias(*, inlet_events=None):
-        for event in inlet_events[DatasetAlias("example-alias-no-taskflow")]:
-            print(event)
-
-    PythonOperator(
-        task_id="consume_dataset_event_from_dataset_alias",
-        python_callable=consume_dataset_event_from_dataset_alias,
-        inlets=[DatasetAlias("example-alias-no-taskflow")],
-    )
diff --git a/example_dags/example_datasets.py b/example_dags/example_datasets.py
deleted file mode 100644
index 54f15d8..0000000
--- a/example_dags/example_datasets.py
+++ /dev/null
@@ -1,192 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG for demonstrating the behavior of the Datasets feature in Airflow, including conditional and
-dataset expression-based scheduling.
-
-Notes on usage:
-
-Turn on all the DAGs.
-
-dataset_produces_1 is scheduled to run daily. Once it completes, it triggers several DAGs due to its dataset
-being updated. dataset_consumes_1 is triggered immediately, as it depends solely on the dataset produced by
-dataset_produces_1. consume_1_or_2_with_dataset_expressions will also be triggered, as its condition of
-either dataset_produces_1 or dataset_produces_2 being updated is satisfied with dataset_produces_1.
-
-dataset_consumes_1_and_2 will not be triggered after dataset_produces_1 runs because it requires the dataset
-from dataset_produces_2, which has no schedule and must be manually triggered.
-
-After manually triggering dataset_produces_2, several DAGs will be affected. dataset_consumes_1_and_2 should
-run because both its dataset dependencies are now met. consume_1_and_2_with_dataset_expressions will be
-triggered, as it requires both dataset_produces_1 and dataset_produces_2 datasets to be updated.
-consume_1_or_2_with_dataset_expressions will be triggered again, since it's conditionally set to run when
-either dataset is updated.
-
-consume_1_or_both_2_and_3_with_dataset_expressions demonstrates complex dataset dependency logic.
-This DAG triggers if dataset_produces_1 is updated or if both dataset_produces_2 and dag3_dataset
-are updated. This example highlights the capability to combine updates from multiple datasets with logical
-expressions for advanced scheduling.
-
-conditional_dataset_and_time_based_timetable illustrates the integration of time-based scheduling with
-dataset dependencies. This DAG is configured to execute either when both dataset_produces_1 and
-dataset_produces_2 datasets have been updated or according to a specific cron schedule, showcasing
-Airflow's versatility in handling mixed triggers for dataset and time-based scheduling.
-
-The DAGs dataset_consumes_1_never_scheduled and dataset_consumes_unknown_never_scheduled will not run
-automatically as they depend on datasets that do not get updated or are not produced by any scheduled tasks.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.datasets import Dataset
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-from airflow.timetables.datasets import DatasetOrTimeSchedule
-from airflow.timetables.trigger import CronTriggerTimetable
-
-# [START dataset_def]
-dag1_dataset = Dataset("s3://dag1/output_1.txt", extra={"hi": "bye"})
-# [END dataset_def]
-dag2_dataset = Dataset("s3://dag2/output_1.txt", extra={"hi": "bye"})
-dag3_dataset = Dataset("s3://dag3/output_3.txt", extra={"hi": "bye"})
-
-with DAG(
-    dag_id="dataset_produces_1",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule="@daily",
-    tags=["produces", "dataset-scheduled"],
-) as dag1:
-    # [START task_outlet]
-    BashOperator(outlets=[dag1_dataset], task_id="producing_task_1", bash_command="sleep 5")
-    # [END task_outlet]
-
-with DAG(
-    dag_id="dataset_produces_2",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=None,
-    tags=["produces", "dataset-scheduled"],
-) as dag2:
-    BashOperator(outlets=[dag2_dataset], task_id="producing_task_2", bash_command="sleep 5")
-
-# [START dag_dep]
-with DAG(
-    dag_id="dataset_consumes_1",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[dag1_dataset],
-    tags=["consumes", "dataset-scheduled"],
-) as dag3:
-    # [END dag_dep]
-    BashOperator(
-        outlets=[Dataset("s3://consuming_1_task/dataset_other.txt")],
-        task_id="consuming_1",
-        bash_command="sleep 5",
-    )
-
-with DAG(
-    dag_id="dataset_consumes_1_and_2",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[dag1_dataset, dag2_dataset],
-    tags=["consumes", "dataset-scheduled"],
-) as dag4:
-    BashOperator(
-        outlets=[Dataset("s3://consuming_2_task/dataset_other_unknown.txt")],
-        task_id="consuming_2",
-        bash_command="sleep 5",
-    )
-
-with DAG(
-    dag_id="dataset_consumes_1_never_scheduled",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[
-        dag1_dataset,
-        Dataset("s3://unrelated/this-dataset-doesnt-get-triggered"),
-    ],
-    tags=["consumes", "dataset-scheduled"],
-) as dag5:
-    BashOperator(
-        outlets=[Dataset("s3://consuming_2_task/dataset_other_unknown.txt")],
-        task_id="consuming_3",
-        bash_command="sleep 5",
-    )
-
-with DAG(
-    dag_id="dataset_consumes_unknown_never_scheduled",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=[
-        Dataset("s3://unrelated/dataset3.txt"),
-        Dataset("s3://unrelated/dataset_other_unknown.txt"),
-    ],
-    tags=["dataset-scheduled"],
-) as dag6:
-    BashOperator(
-        task_id="unrelated_task",
-        outlets=[Dataset("s3://unrelated_task/dataset_other_unknown.txt")],
-        bash_command="sleep 5",
-    )
-
-with DAG(
-    dag_id="consume_1_and_2_with_dataset_expressions",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=(dag1_dataset & dag2_dataset),
-) as dag5:
-    BashOperator(
-        outlets=[Dataset("s3://consuming_2_task/dataset_other_unknown.txt")],
-        task_id="consume_1_and_2_with_dataset_expressions",
-        bash_command="sleep 5",
-    )
-with DAG(
-    dag_id="consume_1_or_2_with_dataset_expressions",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=(dag1_dataset | dag2_dataset),
-) as dag6:
-    BashOperator(
-        outlets=[Dataset("s3://consuming_2_task/dataset_other_unknown.txt")],
-        task_id="consume_1_or_2_with_dataset_expressions",
-        bash_command="sleep 5",
-    )
-with DAG(
-    dag_id="consume_1_or_both_2_and_3_with_dataset_expressions",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=(dag1_dataset | (dag2_dataset & dag3_dataset)),
-) as dag7:
-    BashOperator(
-        outlets=[Dataset("s3://consuming_2_task/dataset_other_unknown.txt")],
-        task_id="consume_1_or_both_2_and_3_with_dataset_expressions",
-        bash_command="sleep 5",
-    )
-with DAG(
-    dag_id="conditional_dataset_and_time_based_timetable",
-    catchup=False,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=DatasetOrTimeSchedule(
-        timetable=CronTriggerTimetable("0 1 * * 3", timezone="UTC"), datasets=(dag1_dataset & dag2_dataset)
-    ),
-    tags=["dataset-time-based-timetable"],
-) as dag8:
-    BashOperator(
-        outlets=[Dataset("s3://dataset_time_based/dataset_other_unknown.txt")],
-        task_id="conditional_dataset_and_time_based_timetable",
-        bash_command="sleep 5",
-    )
diff --git a/example_dags/example_display_name.py b/example_dags/example_display_name.py
deleted file mode 100644
index b6b447d..0000000
--- a/example_dags/example_display_name.py
+++ /dev/null
@@ -1,48 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import dag, task
-from airflow.operators.empty import EmptyOperator
-
-
-# [START dag_decorator_usage]
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-    dag_display_name="Sample DAG with Display Name",
-)
-def example_display_name():
-    sample_task_1 = EmptyOperator(
-        task_id="sample_task_1",
-        task_display_name="Sample Task 1",
-    )
-
-    @task(task_display_name="Sample Task 2")
-    def sample_task_2():
-        pass
-
-    sample_task_1 >> sample_task_2()
-
-
-example_dag = example_display_name()
-# [END dag_decorator_usage]
diff --git a/example_dags/example_dynamic_task_mapping.py b/example_dags/example_dynamic_task_mapping.py
deleted file mode 100644
index 03a77b0..0000000
--- a/example_dags/example_dynamic_task_mapping.py
+++ /dev/null
@@ -1,39 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of dynamic task mapping."""
-
-from __future__ import annotations
-
-from datetime import datetime
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-
-with DAG(dag_id="example_dynamic_task_mapping", schedule=None, start_date=datetime(2022, 3, 4)) as dag:
-
-    @task
-    def add_one(x: int):
-        return x + 1
-
-    @task
-    def sum_it(values):
-        total = sum(values)
-        print(f"Total was {total}")
-
-    added_values = add_one.expand(x=[1, 2, 3])
-    sum_it(added_values)
diff --git a/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py b/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
deleted file mode 100644
index 3d42ac4..0000000
--- a/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
+++ /dev/null
@@ -1,64 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of dynamic task mapping with non-TaskFlow operators."""
-
-from __future__ import annotations
-
-from datetime import datetime
-
-from airflow.models.baseoperator import BaseOperator
-from airflow.models.dag import DAG
-
-
-class AddOneOperator(BaseOperator):
-    """A custom operator that adds one to the input."""
-
-    def __init__(self, value, **kwargs):
-        super().__init__(**kwargs)
-        self.value = value
-
-    def execute(self, context):
-        return self.value + 1
-
-
-class SumItOperator(BaseOperator):
-    """A custom operator that sums the input."""
-
-    template_fields = ("values",)
-
-    def __init__(self, values, **kwargs):
-        super().__init__(**kwargs)
-        self.values = values
-
-    def execute(self, context):
-        total = sum(self.values)
-        print(f"Total was {total}")
-        return total
-
-
-with DAG(
-    dag_id="example_dynamic_task_mapping_with_no_taskflow_operators",
-    schedule=None,
-    start_date=datetime(2022, 3, 4),
-    catchup=False,
-):
-    # map the task to a list of values
-    add_one_task = AddOneOperator.partial(task_id="add_one").expand(value=[1, 2, 3])
-
-    # aggregate (reduce) the mapped tasks results
-    sum_it_task = SumItOperator(task_id="sum_it", values=add_one_task.output)
diff --git a/example_dags/example_external_task_marker_dag.py b/example_dags/example_external_task_marker_dag.py
deleted file mode 100644
index abaf217..0000000
--- a/example_dags/example_external_task_marker_dag.py
+++ /dev/null
@@ -1,98 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating setting up inter-DAG dependencies using ExternalTaskSensor and
-ExternalTaskMarker.
-
-In this example, child_task1 in example_external_task_marker_child depends on parent_task in
-example_external_task_marker_parent. When parent_task is cleared with 'Recursive' selected,
-the presence of ExternalTaskMarker tells Airflow to clear child_task1 and its downstream tasks.
-
-ExternalTaskSensor will keep poking for the status of remote ExternalTaskMarker task at a regular
-interval till one of the following will happen:
-
-ExternalTaskMarker reaches the states mentioned in the allowed_states list.
-In this case, ExternalTaskSensor will exit with a success status code
-
-ExternalTaskMarker reaches the states mentioned in the failed_states list
-In this case, ExternalTaskSensor will raise an AirflowException and user need to handle this
-with multiple downstream tasks
-
-ExternalTaskSensor times out. In this case, ExternalTaskSensor will raise AirflowSkipException
-or AirflowSensorTimeout exception
-
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.sensors.external_task import ExternalTaskMarker, ExternalTaskSensor
-
-start_date = pendulum.datetime(2021, 1, 1, tz="UTC")
-
-with DAG(
-    dag_id="example_external_task_marker_parent",
-    start_date=start_date,
-    catchup=False,
-    schedule=None,
-    tags=["example2"],
-) as parent_dag:
-    # [START howto_operator_external_task_marker]
-    parent_task = ExternalTaskMarker(
-        task_id="parent_task",
-        external_dag_id="example_external_task_marker_child",
-        external_task_id="child_task1",
-    )
-    # [END howto_operator_external_task_marker]
-
-with DAG(
-    dag_id="example_external_task_marker_child",
-    start_date=start_date,
-    schedule=None,
-    catchup=False,
-    tags=["example2"],
-) as child_dag:
-    # [START howto_operator_external_task_sensor]
-    child_task1 = ExternalTaskSensor(
-        task_id="child_task1",
-        external_dag_id=parent_dag.dag_id,
-        external_task_id=parent_task.task_id,
-        timeout=600,
-        allowed_states=["success"],
-        failed_states=["failed", "skipped"],
-        mode="reschedule",
-    )
-    # [END howto_operator_external_task_sensor]
-
-    # [START howto_operator_external_task_sensor_with_task_group]
-    child_task2 = ExternalTaskSensor(
-        task_id="child_task2",
-        external_dag_id=parent_dag.dag_id,
-        external_task_group_id="parent_dag_task_group_id",
-        timeout=600,
-        allowed_states=["success"],
-        failed_states=["failed", "skipped"],
-        mode="reschedule",
-    )
-    # [END howto_operator_external_task_sensor_with_task_group]
-
-    child_task3 = EmptyOperator(task_id="child_task3")
-    child_task1 >> child_task2 >> child_task3
diff --git a/example_dags/example_inlet_event_extra.py b/example_dags/example_inlet_event_extra.py
deleted file mode 100644
index 4b7567f..0000000
--- a/example_dags/example_inlet_event_extra.py
+++ /dev/null
@@ -1,61 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-"""
-Example DAG to demonstrate reading dataset events annotated with extra information.
-
-Also see examples in ``example_outlet_event_extra.py``.
-"""
-
-from __future__ import annotations
-
-import datetime
-
-from airflow.datasets import Dataset
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-
-ds = Dataset("s3://output/1.txt")
-
-with DAG(
-    dag_id="read_dataset_event",
-    catchup=False,
-    start_date=datetime.datetime.min,
-    schedule="@daily",
-    tags=["consumes"],
-):
-
-    @task(inlets=[ds])
-    def read_dataset_event(*, inlet_events=None):
-        for event in inlet_events[ds][:-2]:
-            print(event.extra["hi"])
-
-    read_dataset_event()
-
-with DAG(
-    dag_id="read_dataset_event_from_classic",
-    catchup=False,
-    start_date=datetime.datetime.min,
-    schedule="@daily",
-    tags=["consumes"],
-):
-    BashOperator(
-        task_id="read_dataset_event_from_classic",
-        inlets=[ds],
-        bash_command="echo '{{ inlet_events['s3://output/1.txt'][-1].extra | tojson }}'",
-    )
diff --git a/example_dags/example_kubernetes_executor.py b/example_dags/example_kubernetes_executor.py
deleted file mode 100644
index 395398e..0000000
--- a/example_dags/example_kubernetes_executor.py
+++ /dev/null
@@ -1,239 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-This is an example dag for using a Kubernetes Executor Configuration.
-"""
-
-from __future__ import annotations
-
-import logging
-import os
-
-import pendulum
-
-from airflow.configuration import conf
-from airflow.decorators import task
-from airflow.example_dags.libs.helper import print_stuff
-from airflow.models.dag import DAG
-
-log = logging.getLogger(__name__)
-
-try:
-    from kubernetes.client import models as k8s
-except ImportError:
-    log.warning(
-        "The example_kubernetes_executor example DAG requires the kubernetes provider."
-        " Please install it with: pip install apache-airflow[cncf.kubernetes]"
-    )
-    k8s = None
-
-
-if k8s:
-    with DAG(
-        dag_id="example_kubernetes_executor",
-        schedule=None,
-        start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-        catchup=False,
-        tags=["example3"],
-    ) as dag:
-        # You can use annotations on your kubernetes pods!
-        start_task_executor_config = {
-            "pod_override": k8s.V1Pod(metadata=k8s.V1ObjectMeta(annotations={"test": "annotation"}))
-        }
-
-        @task(executor_config=start_task_executor_config)
-        def start_task():
-            print_stuff()
-
-        # [START task_with_volume]
-        executor_config_volume_mount = {
-            "pod_override": k8s.V1Pod(
-                spec=k8s.V1PodSpec(
-                    containers=[
-                        k8s.V1Container(
-                            name="base",
-                            volume_mounts=[
-                                k8s.V1VolumeMount(mount_path="/foo/", name="example-kubernetes-test-volume")
-                            ],
-                        )
-                    ],
-                    volumes=[
-                        k8s.V1Volume(
-                            name="example-kubernetes-test-volume",
-                            host_path=k8s.V1HostPathVolumeSource(path="/tmp/"),
-                        )
-                    ],
-                )
-            ),
-        }
-
-        @task(executor_config=executor_config_volume_mount)
-        def test_volume_mount():
-            """
-            Tests whether the volume has been mounted.
-            """
-
-            with open("/foo/volume_mount_test.txt", "w") as foo:
-                foo.write("Hello")
-
-            return_code = os.system("cat /foo/volume_mount_test.txt")
-            if return_code != 0:
-                raise ValueError(f"Error when checking volume mount. Return code {return_code}")
-
-        volume_task = test_volume_mount()
-        # [END task_with_volume]
-
-        # [START task_with_sidecar]
-        executor_config_sidecar = {
-            "pod_override": k8s.V1Pod(
-                spec=k8s.V1PodSpec(
-                    containers=[
-                        k8s.V1Container(
-                            name="base",
-                            volume_mounts=[k8s.V1VolumeMount(mount_path="/shared/", name="shared-empty-dir")],
-                        ),
-                        k8s.V1Container(
-                            name="sidecar",
-                            image="ubuntu",
-                            args=['echo "retrieved from mount" > /shared/test.txt'],
-                            command=["bash", "-cx"],
-                            volume_mounts=[k8s.V1VolumeMount(mount_path="/shared/", name="shared-empty-dir")],
-                        ),
-                    ],
-                    volumes=[
-                        k8s.V1Volume(name="shared-empty-dir", empty_dir=k8s.V1EmptyDirVolumeSource()),
-                    ],
-                )
-            ),
-        }
-
-        @task(executor_config=executor_config_sidecar)
-        def test_sharedvolume_mount():
-            """
-            Tests whether the volume has been mounted.
-            """
-            for i in range(5):
-                try:
-                    return_code = os.system("cat /shared/test.txt")
-                    if return_code != 0:
-                        raise ValueError(f"Error when checking volume mount. Return code {return_code}")
-                except ValueError as e:
-                    if i > 4:
-                        raise e
-
-        sidecar_task = test_sharedvolume_mount()
-        # [END task_with_sidecar]
-
-        # You can add labels to pods
-        executor_config_non_root = {
-            "pod_override": k8s.V1Pod(metadata=k8s.V1ObjectMeta(labels={"release": "stable"}))
-        }
-
-        @task(executor_config=executor_config_non_root)
-        def non_root_task():
-            print_stuff()
-
-        third_task = non_root_task()
-
-        executor_config_other_ns = {
-            "pod_override": k8s.V1Pod(
-                metadata=k8s.V1ObjectMeta(namespace="test-namespace", labels={"release": "stable"})
-            )
-        }
-
-        @task(executor_config=executor_config_other_ns)
-        def other_namespace_task():
-            print_stuff()
-
-        other_ns_task = other_namespace_task()
-        worker_container_repository = conf.get("kubernetes_executor", "worker_container_repository")
-        worker_container_tag = conf.get("kubernetes_executor", "worker_container_tag")
-
-        # You can also change the base image, here we used the worker image for demonstration.
-        # Note that the image must have the same configuration as the
-        # worker image. Could be that you want to run this task in a special docker image that has a zip
-        # library built-in. You build the special docker image on top your worker image.
-        kube_exec_config_special = {
-            "pod_override": k8s.V1Pod(
-                spec=k8s.V1PodSpec(
-                    containers=[
-                        k8s.V1Container(
-                            name="base", image=f"{worker_container_repository}:{worker_container_tag}"
-                        ),
-                    ]
-                )
-            )
-        }
-
-        @task(executor_config=kube_exec_config_special)
-        def base_image_override_task():
-            print_stuff()
-
-        base_image_task = base_image_override_task()
-
-        # Use k8s_client.V1Affinity to define node affinity
-        k8s_affinity = k8s.V1Affinity(
-            pod_anti_affinity=k8s.V1PodAntiAffinity(
-                required_during_scheduling_ignored_during_execution=[
-                    k8s.V1PodAffinityTerm(
-                        label_selector=k8s.V1LabelSelector(
-                            match_expressions=[
-                                k8s.V1LabelSelectorRequirement(key="app", operator="In", values=["airflow"])
-                            ]
-                        ),
-                        topology_key="kubernetes.io/hostname",
-                    )
-                ]
-            )
-        )
-
-        # Use k8s_client.V1Toleration to define node tolerations
-        k8s_tolerations = [k8s.V1Toleration(key="dedicated", operator="Equal", value="airflow")]
-
-        # Use k8s_client.V1ResourceRequirements to define resource limits
-        k8s_resource_requirements = k8s.V1ResourceRequirements(
-            requests={"memory": "512Mi"}, limits={"memory": "512Mi"}
-        )
-
-        kube_exec_config_resource_limits = {
-            "pod_override": k8s.V1Pod(
-                spec=k8s.V1PodSpec(
-                    containers=[
-                        k8s.V1Container(
-                            name="base",
-                            resources=k8s_resource_requirements,
-                        )
-                    ],
-                    affinity=k8s_affinity,
-                    tolerations=k8s_tolerations,
-                )
-            )
-        }
-
-        @task(executor_config=kube_exec_config_resource_limits)
-        def task_with_resource_limits():
-            print_stuff()
-
-        four_task = task_with_resource_limits()
-
-        (
-            start_task()
-            >> [volume_task, other_ns_task, sidecar_task]
-            >> third_task
-            >> [base_image_task, four_task]
-        )
diff --git a/example_dags/example_latest_only.py b/example_dags/example_latest_only.py
deleted file mode 100644
index 1e99062..0000000
--- a/example_dags/example_latest_only.py
+++ /dev/null
@@ -1,38 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example of the LatestOnlyOperator"""
-
-from __future__ import annotations
-
-import datetime
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.operators.latest_only import LatestOnlyOperator
-
-with DAG(
-    dag_id="latest_only",
-    schedule=datetime.timedelta(hours=4),
-    start_date=datetime.datetime(2021, 1, 1),
-    catchup=False,
-    tags=["example2", "example3"],
-) as dag:
-    latest_only = LatestOnlyOperator(task_id="latest_only")
-    task1 = EmptyOperator(task_id="task1")
-
-    latest_only >> task1
diff --git a/example_dags/example_latest_only_with_trigger.py b/example_dags/example_latest_only_with_trigger.py
deleted file mode 100644
index 44a5ed4..0000000
--- a/example_dags/example_latest_only_with_trigger.py
+++ /dev/null
@@ -1,49 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example LatestOnlyOperator and TriggerRule interactions
-"""
-
-from __future__ import annotations
-
-# [START example]
-import datetime
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.operators.latest_only import LatestOnlyOperator
-from airflow.utils.trigger_rule import TriggerRule
-
-with DAG(
-    dag_id="latest_only_with_trigger",
-    schedule=datetime.timedelta(hours=4),
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example3"],
-) as dag:
-    latest_only = LatestOnlyOperator(task_id="latest_only")
-    task1 = EmptyOperator(task_id="task1")
-    task2 = EmptyOperator(task_id="task2")
-    task3 = EmptyOperator(task_id="task3")
-    task4 = EmptyOperator(task_id="task4", trigger_rule=TriggerRule.ALL_DONE)
-
-    latest_only >> task1 >> [task3, task4]
-    task2 >> [task3, task4]
-# [END example]
diff --git a/example_dags/example_local_kubernetes_executor.py b/example_dags/example_local_kubernetes_executor.py
deleted file mode 100644
index 07be5ba..0000000
--- a/example_dags/example_local_kubernetes_executor.py
+++ /dev/null
@@ -1,72 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-This is an example dag for using a Local Kubernetes Executor Configuration.
-"""
-
-from __future__ import annotations
-
-import logging
-from datetime import datetime
-
-from airflow.configuration import conf
-from airflow.decorators import task
-from airflow.example_dags.libs.helper import print_stuff
-from airflow.models.dag import DAG
-
-log = logging.getLogger(__name__)
-
-worker_container_repository = conf.get("kubernetes_executor", "worker_container_repository")
-worker_container_tag = conf.get("kubernetes_executor", "worker_container_tag")
-
-try:
-    from kubernetes.client import models as k8s
-except ImportError:
-    log.warning("Could not import DAGs in example_local_kubernetes_executor.py", exc_info=True)
-    log.warning("Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]")
-    k8s = None
-
-if k8s:
-    with DAG(
-        dag_id="example_local_kubernetes_executor",
-        schedule=None,
-        start_date=datetime(2021, 1, 1),
-        catchup=False,
-        tags=["example3"],
-    ) as dag:
-        # You can use annotations on your kubernetes pods!
-        start_task_executor_config = {
-            "pod_override": k8s.V1Pod(metadata=k8s.V1ObjectMeta(annotations={"test": "annotation"}))
-        }
-
-        @task(
-            executor_config=start_task_executor_config,
-            queue="kubernetes",
-            task_id="task_with_kubernetes_executor",
-        )
-        def task_with_template():
-            print_stuff()
-
-        @task(task_id="task_with_local_executor")
-        def task_with_local(ds=None, **kwargs):
-            """Print the Airflow context and ds variable from the context."""
-            print(kwargs)
-            print(ds)
-            return "Whatever you return gets printed in the logs"
-
-        task_with_local() >> task_with_template()
diff --git a/example_dags/example_nested_branch_dag.py b/example_dags/example_nested_branch_dag.py
deleted file mode 100644
index 4e44c3e..0000000
--- a/example_dags/example_nested_branch_dag.py
+++ /dev/null
@@ -1,57 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating a workflow with nested branching. The join tasks are created with
-``none_failed_min_one_success`` trigger rule such that they are skipped whenever their corresponding
-branching tasks are skipped.
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.utils.trigger_rule import TriggerRule
-
-with DAG(
-    dag_id="example_nested_branch_dag",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    schedule="@daily",
-    tags=["example"],
-) as dag:
-
-    @task.branch()
-    def branch(task_id_to_return: str) -> str:
-        return task_id_to_return
-
-    branch_1 = branch.override(task_id="branch_1")(task_id_to_return="true_1")
-    join_1 = EmptyOperator(task_id="join_1", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)
-    true_1 = EmptyOperator(task_id="true_1")
-    false_1 = EmptyOperator(task_id="false_1")
-
-    branch_2 = branch.override(task_id="branch_2")(task_id_to_return="true_2")
-    join_2 = EmptyOperator(task_id="join_2", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)
-    true_2 = EmptyOperator(task_id="true_2")
-    false_2 = EmptyOperator(task_id="false_2")
-    false_3 = EmptyOperator(task_id="false_3")
-
-    branch_1 >> true_1 >> join_1
-    branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1
diff --git a/example_dags/example_outlet_event_extra.py b/example_dags/example_outlet_event_extra.py
deleted file mode 100644
index 5f7d986..0000000
--- a/example_dags/example_outlet_event_extra.py
+++ /dev/null
@@ -1,80 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-"""
-Example DAG to demonstrate annotating a dataset event with extra information.
-
-Also see examples in ``example_inlet_event_extra.py``.
-"""
-
-from __future__ import annotations
-
-import datetime
-
-from airflow.datasets import Dataset
-from airflow.datasets.metadata import Metadata
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-
-ds = Dataset("s3://output/1.txt")
-
-with DAG(
-    dag_id="dataset_with_extra_by_yield",
-    catchup=False,
-    start_date=datetime.datetime.min,
-    schedule="@daily",
-    tags=["produces"],
-):
-
-    @task(outlets=[ds])
-    def dataset_with_extra_by_yield():
-        yield Metadata(ds, {"hi": "bye"})
-
-    dataset_with_extra_by_yield()
-
-with DAG(
-    dag_id="dataset_with_extra_by_context",
-    catchup=False,
-    start_date=datetime.datetime.min,
-    schedule="@daily",
-    tags=["produces"],
-):
-
-    @task(outlets=[ds])
-    def dataset_with_extra_by_context(*, outlet_events=None):
-        outlet_events[ds].extra = {"hi": "bye"}
-
-    dataset_with_extra_by_context()
-
-with DAG(
-    dag_id="dataset_with_extra_from_classic_operator",
-    catchup=False,
-    start_date=datetime.datetime.min,
-    schedule="@daily",
-    tags=["produces"],
-):
-
-    def _dataset_with_extra_from_classic_operator_post_execute(context, result):
-        context["outlet_events"][ds].extra = {"hi": "bye"}
-
-    BashOperator(
-        task_id="dataset_with_extra_from_classic_operator",
-        outlets=[ds],
-        bash_command=":",
-        post_execute=_dataset_with_extra_from_classic_operator_post_execute,
-    )
diff --git a/example_dags/example_params_trigger_ui.py b/example_dags/example_params_trigger_ui.py
deleted file mode 100644
index e47ceae..0000000
--- a/example_dags/example_params_trigger_ui.py
+++ /dev/null
@@ -1,104 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage DAG params to model a trigger UI with a user form.
-
-This example DAG generates greetings to a list of provided names in selected languages in the logs.
-"""
-
-from __future__ import annotations
-
-import datetime
-from pathlib import Path
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.models.param import Param, ParamsDict
-from airflow.utils.trigger_rule import TriggerRule
-
-# [START params_trigger]
-with DAG(
-    dag_id=Path(__file__).stem,
-    dag_display_name="Params Trigger UI",
-    description=__doc__.partition(".")[0],
-    doc_md=__doc__,
-    schedule=None,
-    start_date=datetime.datetime(2022, 3, 4),
-    catchup=False,
-    tags=["example", "params"],
-    params={
-        "names": Param(
-            ["Linda", "Martha", "Thomas"],
-            type="array",
-            description="Define the list of names for which greetings should be generated in the logs."
-            " Please have one name per line.",
-            title="Names to greet",
-        ),
-        "english": Param(True, type="boolean", title="English"),
-        "german": Param(True, type="boolean", title="German (Formal)"),
-        "french": Param(True, type="boolean", title="French"),
-    },
-) as dag:
-
-    @task(task_id="get_names", task_display_name="Get names")
-    def get_names(**kwargs) -> list[str]:
-        params: ParamsDict = kwargs["params"]
-        if "names" not in params:
-            print("Uuups, no names given, was no UI used to trigger?")
-            return []
-        return params["names"]
-
-    @task.branch(task_id="select_languages", task_display_name="Select languages")
-    def select_languages(**kwargs) -> list[str]:
-        params: ParamsDict = kwargs["params"]
-        selected_languages = []
-        for lang in ["english", "german", "french"]:
-            if params[lang]:
-                selected_languages.append(f"generate_{lang}_greeting")
-        return selected_languages
-
-    @task(task_id="generate_english_greeting", task_display_name="Generate English greeting")
-    def generate_english_greeting(name: str) -> str:
-        return f"Hello {name}!"
-
-    @task(task_id="generate_german_greeting", task_display_name="Erzeuge Deutsche Begrung")
-    def generate_german_greeting(name: str) -> str:
-        return f"Sehr geehrter Herr/Frau {name}."
-
-    @task(task_id="generate_french_greeting", task_display_name="Produire un message d'accueil en franais")
-    def generate_french_greeting(name: str) -> str:
-        return f"Bonjour {name}!"
-
-    @task(task_id="print_greetings", task_display_name="Print greetings", trigger_rule=TriggerRule.ALL_DONE)
-    def print_greetings(greetings1, greetings2, greetings3) -> None:
-        for g in greetings1 or []:
-            print(g)
-        for g in greetings2 or []:
-            print(g)
-        for g in greetings3 or []:
-            print(g)
-        if not (greetings1 or greetings2 or greetings3):
-            print("sad, nobody to greet :-(")
-
-    lang_select = select_languages()
-    names = get_names()
-    english_greetings = generate_english_greeting.expand(name=names)
-    german_greetings = generate_german_greeting.expand(name=names)
-    french_greetings = generate_french_greeting.expand(name=names)
-    lang_select >> [english_greetings, german_greetings, french_greetings]
-    results_print = print_greetings(english_greetings, german_greetings, french_greetings)
-# [END params_trigger]
diff --git a/example_dags/example_params_ui_tutorial.py b/example_dags/example_params_ui_tutorial.py
deleted file mode 100644
index 133df85..0000000
--- a/example_dags/example_params_ui_tutorial.py
+++ /dev/null
@@ -1,257 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""DAG demonstrating various options for a trigger form generated by DAG params.
-
-The DAG attribute `params` is used to define a default dictionary of parameters which are usually passed
-to the DAG and which are used to render a trigger form.
-"""
-
-from __future__ import annotations
-
-import datetime
-import json
-from pathlib import Path
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.models.param import Param, ParamsDict
-
-with DAG(
-    dag_id=Path(__file__).stem,
-    dag_display_name="Params UI tutorial",
-    description=__doc__.partition(".")[0],
-    doc_md=__doc__,
-    schedule=None,
-    start_date=datetime.datetime(2022, 3, 4),
-    catchup=False,
-    tags=["example", "params", "ui"],
-    # [START section_1]
-    params={
-        # Let's start simple: Standard dict values are detected from type and offered as entry form fields.
-        # Detected types are numbers, text, boolean, lists and dicts.
-        # Note that such auto-detected parameters are treated as optional (not required to contain a value)
-        "x": 3,
-        "text": "Hello World!",
-        "flag": False,
-        "a_simple_list": ["one", "two", "three", "actually one value is made per line"],
-        # But of course you might want to have it nicer! Let's add some description to parameters.
-        # Note if you can add any Markdown formatting to the description, you need to use the description_md
-        # attribute.
-        "most_loved_number": Param(
-            42,
-            type="integer",
-            title="Your favorite number",
-            description_md="Everybody should have a **favorite** number. Not only _math teachers_. "
-            "If you can not think of any at the moment please think of the 42 which is very famous because"
-            "of the book [The Hitchhiker's Guide to the Galaxy]"
-            "(https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#"
-            "The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42).",
-        ),
-        # If you want to have a selection list box then you can use the enum feature of JSON schema
-        "pick_one": Param(
-            "value 42",
-            type="string",
-            title="Select one Value",
-            description="You can use JSON schema enum's to generate drop down selection boxes.",
-            enum=[f"value {i}" for i in range(16, 64)],
-        ),
-        # [END section_1]
-        # You can also label the selected values via values_display attribute
-        "pick_with_label": Param(
-            3,
-            type="number",
-            title="Select one Number",
-            description="With drop down selections you can also have nice display labels for the values.",
-            enum=[*range(1, 10)],
-            values_display={
-                1: "One",
-                2: "Two",
-                3: "Three",
-                4: "Four - is like you take three and get one for free!",
-                5: "Five",
-                6: "Six",
-                7: "Seven",
-                8: "Eight",
-                9: "Nine",
-            },
-        ),
-        # If you want to have a list box with proposals but not enforcing a fixed list
-        # then you can use the examples feature of JSON schema
-        "proposals": Param(
-            "some value",
-            type="string",
-            title="Field with proposals",
-            description="You can use JSON schema examples's to generate drop down selection boxes "
-            "but allow also to enter custom values. Try typing an 'a' and see options.",
-            examples=(
-                "Alpha,Bravo,Charlie,Delta,Echo,Foxtrot,Golf,Hotel,India,Juliett,Kilo,Lima,Mike,November,Oscar,Papa,"
-                "Quebec,Romeo,Sierra,Tango,Uniform,Victor,Whiskey,X-ray,Yankee,Zulu"
-            ).split(","),
-        ),
-        # If you want to select multiple items from a fixed list JSON schema does not allow to use enum
-        # In this case the type "array" is being used together with "examples" as pick list
-        "multi_select": Param(
-            ["two", "three"],
-            "Select from the list of options.",
-            type="array",
-            title="Multi Select",
-            examples=["one", "two", "three", "four", "five"],
-        ),
-        # A multiple options selection can also be combined with values_display
-        "multi_select_with_label": Param(
-            ["2", "3"],
-            "Select from the list of options. See that options can have nicer text and still technical values"
-            "are propagated as values during trigger to the DAG.",
-            type="array",
-            title="Multi Select with Labels",
-            examples=["1", "2", "3", "4", "5"],
-            values_display={
-                "1": "One box of choccolate",
-                "2": "Two bananas",
-                "3": "Three apples",
-                # Note: Value display mapping does not need to be complete
-            },
-        ),
-        # An array of numbers
-        "array_of_numbers": Param(
-            [1, 2, 3],
-            "Only integers are accepted in this array",
-            type="array",
-            title="Array of numbers",
-            items={"type": "number"},
-        ),
-        # Boolean as proper parameter with description
-        "bool": Param(
-            True,
-            type="boolean",
-            title="Please confirm",
-            description="A On/Off selection with a proper description.",
-        ),
-        # Dates and Times are also supported
-        "date_time": Param(
-            f"{datetime.date.today()}T{datetime.time(hour=12, minute=17, second=00)}+00:00",
-            type="string",
-            format="date-time",
-            title="Date-Time Picker",
-            description="Please select a date and time, use the button on the left for a pop-up calendar.",
-        ),
-        "date": Param(
-            f"{datetime.date.today()}",
-            type="string",
-            format="date",
-            title="Date Picker",
-            description="Please select a date, use the button on the left for a pop-up calendar. "
-            "See that here are no times!",
-        ),
-        "time": Param(
-            f"{datetime.time(hour=12, minute=13, second=14)}",
-            type=["string", "null"],
-            format="time",
-            title="Time Picker",
-            description="Please select a time, use the button on the left for a pop-up tool.",
-        ),
-        "multiline_text": Param(
-            "A multiline text Param\nthat will keep the newline\ncharacters in its value.",
-            description="This field allows for multiline text input. The returned value will be a single with newline (\\n) characters kept intact.",
-            type=["string", "null"],
-            format="multiline",
-        ),
-        # Fields can be required or not. If the defined fields are typed they are getting required by default
-        # (else they would not pass JSON schema validation) - to make typed fields optional you must
-        # permit the optional "null" type.
-        # You can omit a default value if the DAG is triggered manually
-        # [START section_2]
-        "required_field": Param(
-            # In this example we have no default value
-            # Form will enforce a value supplied by users to be able to trigger
-            type="string",
-            title="Required text field",
-            description="This field is required. You can not submit without having text in here.",
-        ),
-        "optional_field": Param(
-            "optional text, you can trigger also w/o text",
-            type=["null", "string"],
-            title="Optional text field",
-            description_md="This field is optional. As field content is JSON schema validated you must "
-            "allow the `null` type.",
-        ),
-        # [END section_2]
-        # You can arrange the entry fields in sections so that you can have a better overview for the user
-        # Therefore you can add the "section" attribute.
-        # The benefit of the Params class definition is that the full scope of JSON schema validation
-        # can be leveraged for form fields and they will be validated before DAG submission.
-        "checked_text": Param(
-            "length-checked-field",
-            type="string",
-            title="Text field with length check",
-            description_md="""This field is required. And you need to provide something between 10 and 30
-            characters. See the JSON
-            [schema description (string)](https://json-schema.org/understanding-json-schema/reference/string.html)
-            for more details""",
-            minLength=10,
-            maxLength=20,
-            section="JSON Schema validation options",
-        ),
-        "checked_number": Param(
-            100,
-            type="number",
-            title="Number field with value check",
-            description_md="""This field is required. You need to provide any number between 64 and 128.
-            See the JSON
-            [schema description (numbers)](https://json-schema.org/understanding-json-schema/reference/numeric.html)
-            for more details""",
-            minimum=64,
-            maximum=128,
-            section="JSON Schema validation options",
-        ),
-        # Some further cool stuff as advanced options are also possible
-        # You can have the user entering a dict object as a JSON with validation
-        "object": Param(
-            {"key": "value"},
-            type=["object", "null"],
-            title="JSON entry field",
-            section="Special advanced stuff with form fields",
-        ),
-        "array_of_objects": Param(
-            [{"name": "account_name", "country": "country_name"}],
-            description_md="Array with complex objects and validation rules. "
-            "See [JSON Schema validation options in specs]"
-            "(https://json-schema.org/understanding-json-schema/reference/array.html#items).",
-            type="array",
-            title="JSON array field",
-            items={
-                "type": "object",
-                "properties": {"name": {"type": "string"}, "country_name": {"type": "string"}},
-                "required": ["name"],
-            },
-            section="Special advanced stuff with form fields",
-        ),
-        # If you want to have static parameters which are always passed and not editable by the user
-        # then you can use the JSON schema option of passing constant values. These parameters
-        # will not be displayed but passed to the DAG
-        "hidden_secret_field": Param("constant value", const="constant value"),
-    },
-) as dag:
-    # [START section_3]
-    @task(task_display_name="Show used parameters")
-    def show_params(**kwargs) -> None:
-        params: ParamsDict = kwargs["params"]
-        print(f"This DAG was triggered with the following parameters:\n\n{json.dumps(params, indent=4)}\n")
-
-    show_params()
-# [END section_3]
diff --git a/example_dags/example_passing_params_via_test_command.py b/example_dags/example_passing_params_via_test_command.py
deleted file mode 100644
index 2fcb8e4..0000000
--- a/example_dags/example_passing_params_via_test_command.py
+++ /dev/null
@@ -1,87 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the params arguments in templated arguments."""
-
-from __future__ import annotations
-
-import datetime
-import os
-import textwrap
-
-import pendulum
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-
-
-@task(task_id="run_this")
-def my_py_command(params, test_mode=None, task=None):
-    """
-    Print out the "foo" param passed in via
-    `airflow tasks test example_passing_params_via_test_command run_this <date>
-    -t '{"foo":"bar"}'`
-    """
-    if test_mode:
-        print(
-            f" 'foo' was passed in via test={test_mode} command : kwargs[params][foo] = {task.params['foo']}"
-        )
-    # Print out the value of "miff", passed in below via the Python Operator
-    print(f" 'miff' was passed in via task params = {params['miff']}")
-    return 1
-
-
-@task(task_id="env_var_test_task")
-def print_env_vars(test_mode=None):
-    """
-    Print out the "foo" param passed in via
-    `airflow tasks test example_passing_params_via_test_command env_var_test_task <date>
-    --env-vars '{"foo":"bar"}'`
-    """
-    if test_mode:
-        print(f"foo={os.environ.get('foo')}")
-        print(f"AIRFLOW_TEST_MODE={os.environ.get('AIRFLOW_TEST_MODE')}")
-
-
-with DAG(
-    "example_passing_params_via_test_command",
-    schedule="*/1 * * * *",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    dagrun_timeout=datetime.timedelta(minutes=4),
-    tags=["example"],
-) as dag:
-    run_this = my_py_command(params={"miff": "agg"})
-
-    my_command = textwrap.dedent(
-        """
-        echo "'foo' was passed in via Airflow CLI Test command with value '$FOO'"
-        echo "'miff' was passed in via BashOperator with value '$MIFF'"
-        """
-    )
-
-    also_run_this = BashOperator(
-        task_id="also_run_this",
-        bash_command=my_command,
-        params={"miff": "agg"},
-        env={"FOO": "{{ params.foo }}", "MIFF": "{{ params.miff }}"},
-    )
-
-    env_var_test_task = print_env_vars()
-
-    run_this >> also_run_this
diff --git a/example_dags/example_python_context_decorator.py b/example_dags/example_python_context_decorator.py
deleted file mode 100644
index 497ee08..0000000
--- a/example_dags/example_python_context_decorator.py
+++ /dev/null
@@ -1,92 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of the PythonOperator with `get_current_context()` to get the current context.
-
-Also, demonstrates the usage of the TaskFlow API.
-"""
-
-from __future__ import annotations
-
-import sys
-
-import pendulum
-
-from airflow.decorators import dag, task
-
-SOME_EXTERNAL_PYTHON = sys.executable
-
-
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-)
-def example_python_context_decorator():
-    # [START get_current_context]
-    @task(task_id="print_the_context")
-    def print_context() -> str:
-        """Print the Airflow context."""
-        from pprint import pprint
-
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        pprint(context)
-        return "Whatever you return gets printed in the logs"
-
-    print_the_context = print_context()
-    # [END get_current_context]
-
-    # [START get_current_context_venv]
-    @task.virtualenv(task_id="print_the_context_venv", use_airflow_context=True)
-    def print_context_venv() -> str:
-        """Print the Airflow context in venv."""
-        from pprint import pprint
-
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        pprint(context)
-        return "Whatever you return gets printed in the logs"
-
-    print_the_context_venv = print_context_venv()
-    # [END get_current_context_venv]
-
-    # [START get_current_context_external]
-    @task.external_python(
-        task_id="print_the_context_external", python=SOME_EXTERNAL_PYTHON, use_airflow_context=True
-    )
-    def print_context_external() -> str:
-        """Print the Airflow context in external python."""
-        from pprint import pprint
-
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        pprint(context)
-        return "Whatever you return gets printed in the logs"
-
-    print_the_context_external = print_context_external()
-    # [END get_current_context_external]
-
-    _ = print_the_context >> [print_the_context_venv, print_the_context_external]
-
-
-example_python_context_decorator()
diff --git a/example_dags/example_python_context_operator.py b/example_dags/example_python_context_operator.py
deleted file mode 100644
index f1b76c5..0000000
--- a/example_dags/example_python_context_operator.py
+++ /dev/null
@@ -1,91 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of the PythonOperator with `get_current_context()` to get the current context.
-
-Also, demonstrates the usage of the classic Python operators.
-"""
-
-from __future__ import annotations
-
-import sys
-
-import pendulum
-
-from airflow import DAG
-from airflow.operators.python import ExternalPythonOperator, PythonOperator, PythonVirtualenvOperator
-
-SOME_EXTERNAL_PYTHON = sys.executable
-
-with DAG(
-    dag_id="example_python_context_operator",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    # [START get_current_context]
-    def print_context() -> str:
-        """Print the Airflow context."""
-        from pprint import pprint
-
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        pprint(context)
-        return "Whatever you return gets printed in the logs"
-
-    print_the_context = PythonOperator(task_id="print_the_context", python_callable=print_context)
-    # [END get_current_context]
-
-    # [START get_current_context_venv]
-    def print_context_venv() -> str:
-        """Print the Airflow context in venv."""
-        from pprint import pprint
-
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        pprint(context)
-        return "Whatever you return gets printed in the logs"
-
-    print_the_context_venv = PythonVirtualenvOperator(
-        task_id="print_the_context_venv", python_callable=print_context_venv, use_airflow_context=True
-    )
-    # [END get_current_context_venv]
-
-    # [START get_current_context_external]
-    def print_context_external() -> str:
-        """Print the Airflow context in external python."""
-        from pprint import pprint
-
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        pprint(context)
-        return "Whatever you return gets printed in the logs"
-
-    print_the_context_external = ExternalPythonOperator(
-        task_id="print_the_context_external",
-        python_callable=print_context_external,
-        python=SOME_EXTERNAL_PYTHON,
-        use_airflow_context=True,
-    )
-    # [END get_current_context_external]
-
-    _ = print_the_context >> [print_the_context_venv, print_the_context_external]
diff --git a/example_dags/example_python_decorator.py b/example_dags/example_python_decorator.py
deleted file mode 100644
index 264fc43..0000000
--- a/example_dags/example_python_decorator.py
+++ /dev/null
@@ -1,136 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of the TaskFlow API to execute Python functions natively and within a
-virtual environment.
-"""
-
-from __future__ import annotations
-
-import logging
-import sys
-import time
-from pprint import pprint
-
-import pendulum
-
-from airflow.decorators import dag, task
-from airflow.operators.python import is_venv_installed
-
-log = logging.getLogger(__name__)
-
-PATH_TO_PYTHON_BINARY = sys.executable
-
-
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-)
-def example_python_decorator():
-    # [START howto_operator_python]
-    @task(task_id="print_the_context")
-    def print_context(ds=None, **kwargs):
-        """Print the Airflow context and ds variable from the context."""
-        pprint(kwargs)
-        print(ds)
-        return "Whatever you return gets printed in the logs"
-
-    run_this = print_context()
-    # [END howto_operator_python]
-
-    # [START howto_operator_python_render_sql]
-    @task(task_id="log_sql_query", templates_dict={"query": "sql/sample.sql"}, templates_exts=[".sql"])
-    def log_sql(**kwargs):
-        log.info("Python task decorator query: %s", str(kwargs["templates_dict"]["query"]))
-
-    log_the_sql = log_sql()
-    # [END howto_operator_python_render_sql]
-
-    # [START howto_operator_python_kwargs]
-    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively
-    @task
-    def my_sleeping_function(random_base):
-        """This is a function that will run within the DAG execution"""
-        time.sleep(random_base)
-
-    for i in range(5):
-        sleeping_task = my_sleeping_function.override(task_id=f"sleep_for_{i}")(random_base=i / 10)
-
-        run_this >> log_the_sql >> sleeping_task
-    # [END howto_operator_python_kwargs]
-
-    if not is_venv_installed():
-        log.warning("The virtalenv_python example task requires virtualenv, please install it.")
-    else:
-        # [START howto_operator_python_venv]
-        @task.virtualenv(
-            task_id="virtualenv_python", requirements=["colorama==0.4.0"], system_site_packages=False
-        )
-        def callable_virtualenv():
-            """
-            Example function that will be performed in a virtual environment.
-
-            Importing at the module level ensures that it will not attempt to import the
-            library before it is installed.
-            """
-            from time import sleep
-
-            from colorama import Back, Fore, Style
-
-            print(Fore.RED + "some red text")
-            print(Back.GREEN + "and with a green background")
-            print(Style.DIM + "and in dim text")
-            print(Style.RESET_ALL)
-            for _ in range(4):
-                print(Style.DIM + "Please wait...", flush=True)
-                sleep(1)
-            print("Finished")
-
-        virtualenv_task = callable_virtualenv()
-        # [END howto_operator_python_venv]
-
-        sleeping_task >> virtualenv_task
-
-        # [START howto_operator_external_python]
-        @task.external_python(task_id="external_python", python=PATH_TO_PYTHON_BINARY)
-        def callable_external_python():
-            """
-            Example function that will be performed in a virtual environment.
-
-            Importing at the module level ensures that it will not attempt to import the
-            library before it is installed.
-            """
-            import sys
-            from time import sleep
-
-            print(f"Running task via {sys.executable}")
-            print("Sleeping")
-            for _ in range(4):
-                print("Please wait...", flush=True)
-                sleep(1)
-            print("Finished")
-
-        external_python_task = callable_external_python()
-        # [END howto_operator_external_python]
-
-        run_this >> external_python_task >> virtualenv_task
-
-
-example_python_decorator()
diff --git a/example_dags/example_python_operator.py b/example_dags/example_python_operator.py
deleted file mode 100644
index a1ebb84..0000000
--- a/example_dags/example_python_operator.py
+++ /dev/null
@@ -1,151 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating the usage of the classic Python operators to execute Python functions natively and
-within a virtual environment.
-"""
-
-from __future__ import annotations
-
-import logging
-import sys
-import time
-from pprint import pprint
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.python import (
-    ExternalPythonOperator,
-    PythonOperator,
-    PythonVirtualenvOperator,
-    is_venv_installed,
-)
-
-log = logging.getLogger(__name__)
-
-PATH_TO_PYTHON_BINARY = sys.executable
-
-
-with DAG(
-    dag_id="example_python_operator",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-):
-    # [START howto_operator_python]
-    def print_context(ds=None, **kwargs):
-        """Print the Airflow context and ds variable from the context."""
-        print("::group::All kwargs")
-        pprint(kwargs)
-        print("::endgroup::")
-        print("::group::Context variable ds")
-        print(ds)
-        print("::endgroup::")
-        return "Whatever you return gets printed in the logs"
-
-    run_this = PythonOperator(task_id="print_the_context", python_callable=print_context)
-    # [END howto_operator_python]
-
-    # [START howto_operator_python_render_sql]
-    def log_sql(**kwargs):
-        log.info("Python task decorator query: %s", str(kwargs["templates_dict"]["query"]))
-
-    log_the_sql = PythonOperator(
-        task_id="log_sql_query",
-        python_callable=log_sql,
-        templates_dict={"query": "sql/sample.sql"},
-        templates_exts=[".sql"],
-    )
-    # [END howto_operator_python_render_sql]
-
-    # [START howto_operator_python_kwargs]
-    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively
-    def my_sleeping_function(random_base):
-        """This is a function that will run within the DAG execution"""
-        time.sleep(random_base)
-
-    for i in range(5):
-        sleeping_task = PythonOperator(
-            task_id=f"sleep_for_{i}", python_callable=my_sleeping_function, op_kwargs={"random_base": i / 10}
-        )
-
-        run_this >> log_the_sql >> sleeping_task
-    # [END howto_operator_python_kwargs]
-
-    if not is_venv_installed():
-        log.warning("The virtalenv_python example task requires virtualenv, please install it.")
-    else:
-        # [START howto_operator_python_venv]
-        def callable_virtualenv():
-            """
-            Example function that will be performed in a virtual environment.
-
-            Importing at the function level ensures that it will not attempt to import the
-            library before it is installed.
-            """
-            from time import sleep
-
-            from colorama import Back, Fore, Style
-
-            print(Fore.RED + "some red text")
-            print(Back.GREEN + "and with a green background")
-            print(Style.DIM + "and in dim text")
-            print(Style.RESET_ALL)
-            for _ in range(4):
-                print(Style.DIM + "Please wait...", flush=True)
-                sleep(1)
-            print("Finished")
-
-        virtualenv_task = PythonVirtualenvOperator(
-            task_id="virtualenv_python",
-            python_callable=callable_virtualenv,
-            requirements=["colorama==0.4.0"],
-            system_site_packages=False,
-        )
-        # [END howto_operator_python_venv]
-
-        sleeping_task >> virtualenv_task
-
-        # [START howto_operator_external_python]
-        def callable_external_python():
-            """
-            Example function that will be performed in a virtual environment.
-
-            Importing at the module level ensures that it will not attempt to import the
-            library before it is installed.
-            """
-            import sys
-            from time import sleep
-
-            print(f"Running task via {sys.executable}")
-            print("Sleeping")
-            for _ in range(4):
-                print("Please wait...", flush=True)
-                sleep(1)
-            print("Finished")
-
-        external_python_task = ExternalPythonOperator(
-            task_id="external_python",
-            python_callable=callable_external_python,
-            python=PATH_TO_PYTHON_BINARY,
-        )
-        # [END howto_operator_external_python]
-
-        run_this >> external_python_task >> virtualenv_task
diff --git a/example_dags/example_sensor_decorator.py b/example_dags/example_sensor_decorator.py
deleted file mode 100644
index db30593..0000000
--- a/example_dags/example_sensor_decorator.py
+++ /dev/null
@@ -1,67 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-"""Example DAG demonstrating the usage of the sensor decorator."""
-
-from __future__ import annotations
-
-# [START tutorial]
-# [START import_module]
-import pendulum
-
-from airflow.decorators import dag, task
-from airflow.sensors.base import PokeReturnValue
-
-# [END import_module]
-
-
-# [START instantiate_dag]
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-)
-def example_sensor_decorator():
-    # [END instantiate_dag]
-
-    # [START wait_function]
-    # Using a sensor operator to wait for the upstream data to be ready.
-    @task.sensor(poke_interval=60, timeout=3600, mode="reschedule")
-    def wait_for_upstream() -> PokeReturnValue:
-        return PokeReturnValue(is_done=True, xcom_value="xcom_value")
-
-    # [END wait_function]
-
-    # [START dummy_function]
-    @task
-    def dummy_operator() -> None:
-        pass
-
-    # [END dummy_function]
-
-    # [START main_flow]
-    wait_for_upstream() >> dummy_operator()
-    # [END main_flow]
-
-
-# [START dag_invocation]
-tutorial_etl_dag = example_sensor_decorator()
-# [END dag_invocation]
-
-# [END tutorial]
diff --git a/example_dags/example_sensors.py b/example_dags/example_sensors.py
deleted file mode 100644
index 6fb564e..0000000
--- a/example_dags/example_sensors.py
+++ /dev/null
@@ -1,132 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-from __future__ import annotations
-
-import datetime
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-from airflow.providers.standard.sensors.time import TimeSensor, TimeSensorAsync
-from airflow.providers.standard.sensors.time_delta import TimeDeltaSensor, TimeDeltaSensorAsync
-from airflow.providers.standard.sensors.weekday import DayOfWeekSensor
-from airflow.sensors.bash import BashSensor
-from airflow.sensors.filesystem import FileSensor
-from airflow.sensors.python import PythonSensor
-from airflow.utils.trigger_rule import TriggerRule
-from airflow.utils.weekday import WeekDay
-
-
-# [START example_callables]
-def success_callable():
-    return True
-
-
-def failure_callable():
-    return False
-
-
-# [END example_callables]
-
-
-with DAG(
-    dag_id="example_sensors",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    # [START example_time_delta_sensor]
-    t0 = TimeDeltaSensor(task_id="wait_some_seconds", delta=datetime.timedelta(seconds=2))
-    # [END example_time_delta_sensor]
-
-    # [START example_time_delta_sensor_async]
-    t0a = TimeDeltaSensorAsync(task_id="wait_some_seconds_async", delta=datetime.timedelta(seconds=2))
-    # [END example_time_delta_sensor_async]
-
-    # [START example_time_sensors]
-    t1 = TimeSensor(
-        task_id="fire_immediately", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()
-    )
-
-    t2 = TimeSensor(
-        task_id="timeout_after_second_date_in_the_future",
-        timeout=1,
-        soft_fail=True,
-        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),
-    )
-    # [END example_time_sensors]
-
-    # [START example_time_sensors_async]
-    t1a = TimeSensorAsync(
-        task_id="fire_immediately_async", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()
-    )
-
-    t2a = TimeSensorAsync(
-        task_id="timeout_after_second_date_in_the_future_async",
-        timeout=1,
-        soft_fail=True,
-        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),
-    )
-    # [END example_time_sensors_async]
-
-    # [START example_bash_sensors]
-    t3 = BashSensor(task_id="Sensor_succeeds", bash_command="exit 0")
-
-    t4 = BashSensor(task_id="Sensor_fails_after_3_seconds", timeout=3, soft_fail=True, bash_command="exit 1")
-    # [END example_bash_sensors]
-
-    t5 = BashOperator(task_id="remove_file", bash_command="rm -rf /tmp/temporary_file_for_testing")
-
-    # [START example_file_sensor]
-    t6 = FileSensor(task_id="wait_for_file", filepath="/tmp/temporary_file_for_testing")
-    # [END example_file_sensor]
-
-    # [START example_file_sensor_async]
-    t7 = FileSensor(
-        task_id="wait_for_file_async", filepath="/tmp/temporary_file_for_testing", deferrable=True
-    )
-    # [END example_file_sensor_async]
-
-    t8 = BashOperator(
-        task_id="create_file_after_3_seconds", bash_command="sleep 3; touch /tmp/temporary_file_for_testing"
-    )
-
-    # [START example_python_sensors]
-    t9 = PythonSensor(task_id="success_sensor_python", python_callable=success_callable)
-
-    t10 = PythonSensor(
-        task_id="failure_timeout_sensor_python", timeout=3, soft_fail=True, python_callable=failure_callable
-    )
-    # [END example_python_sensors]
-
-    # [START example_day_of_week_sensor]
-    t11 = DayOfWeekSensor(
-        task_id="week_day_sensor_failing_on_timeout", timeout=3, soft_fail=True, week_day=WeekDay.MONDAY
-    )
-    # [END example_day_of_week_sensor]
-
-    tx = BashOperator(task_id="print_date_in_bash", bash_command="date")
-
-    tx.trigger_rule = TriggerRule.NONE_FAILED
-    [t0, t0a, t1, t1a, t2, t2a, t3, t4] >> tx
-    t5 >> t6 >> t7 >> tx
-    t8 >> tx
-    [t9, t10] >> tx
-    t11 >> tx
diff --git a/example_dags/example_setup_teardown.py b/example_dags/example_setup_teardown.py
deleted file mode 100644
index 9fab87d..0000000
--- a/example_dags/example_setup_teardown.py
+++ /dev/null
@@ -1,50 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of setup and teardown tasks."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-from airflow.utils.task_group import TaskGroup
-
-with DAG(
-    dag_id="example_setup_teardown",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    root_setup = BashOperator(task_id="root_setup", bash_command="echo 'Hello from root_setup'").as_setup()
-    root_normal = BashOperator(task_id="normal", bash_command="echo 'I am just a normal task'")
-    root_teardown = BashOperator(
-        task_id="root_teardown", bash_command="echo 'Goodbye from root_teardown'"
-    ).as_teardown(setups=root_setup)
-    root_setup >> root_normal >> root_teardown
-    with TaskGroup("section_1") as section_1:
-        inner_setup = BashOperator(
-            task_id="taskgroup_setup", bash_command="echo 'Hello from taskgroup_setup'"
-        ).as_setup()
-        inner_normal = BashOperator(task_id="normal", bash_command="echo 'I am just a normal task'")
-        inner_teardown = BashOperator(
-            task_id="taskgroup_teardown", bash_command="echo 'Hello from taskgroup_teardown'"
-        ).as_teardown(setups=inner_setup)
-        inner_setup >> inner_normal >> inner_teardown
-    root_normal >> section_1
diff --git a/example_dags/example_setup_teardown_taskflow.py b/example_dags/example_setup_teardown_taskflow.py
deleted file mode 100644
index 6fec9f9..0000000
--- a/example_dags/example_setup_teardown_taskflow.py
+++ /dev/null
@@ -1,107 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of setup and teardown tasks."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import setup, task, task_group, teardown
-from airflow.models.dag import DAG
-
-with DAG(
-    dag_id="example_setup_teardown_taskflow",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-
-    @task
-    def my_first_task():
-        print("Hello 1")
-
-    @task
-    def my_second_task():
-        print("Hello 2")
-
-    @task
-    def my_third_task():
-        print("Hello 3")
-
-    # you can set setup / teardown relationships with the `as_teardown` method.
-    task_1 = my_first_task()
-    task_2 = my_second_task()
-    task_3 = my_third_task()
-    task_1 >> task_2 >> task_3.as_teardown(setups=task_1)
-
-    # The method `as_teardown` will mark task_3 as teardown, task_1 as setup, and
-    # arrow task_1 >> task_3.
-    # Now if you clear task_2, then its setup task, task_1, will be cleared in
-    # addition to its teardown task, task_3
-
-    # it's also possible to use a decorator to mark a task as setup or
-    # teardown when you define it. see below.
-
-    @setup
-    def outer_setup():
-        print("I am outer_setup")
-        return "some cluster id"
-
-    @teardown
-    def outer_teardown(cluster_id):
-        print("I am outer_teardown")
-        print(f"Tearing down cluster: {cluster_id}")
-
-    @task
-    def outer_work():
-        print("I am just a normal task")
-
-    @task_group
-    def section_1():
-        @setup
-        def inner_setup():
-            print("I set up")
-            return "some_cluster_id"
-
-        @task
-        def inner_work(cluster_id):
-            print(f"doing some work with {cluster_id=}")
-
-        @teardown
-        def inner_teardown(cluster_id):
-            print(f"tearing down {cluster_id=}")
-
-        # this passes the return value of `inner_setup` to both `inner_work` and `inner_teardown`
-        inner_setup_task = inner_setup()
-        inner_work(inner_setup_task) >> inner_teardown(inner_setup_task)
-
-    # by using the decorators, outer_setup and outer_teardown are already marked as setup / teardown
-    # now we just need to make sure they are linked directly.  At a low level, what we need
-    # to do so is the following::
-    #     s = outer_setup()
-    #     t = outer_teardown()
-    #     s >> t
-    #     s >> outer_work() >> t
-    # Thus, s and t are linked directly, and outer_work runs in between.  We can take advantage of
-    # the fact that we are in taskflow, along with the context manager on teardowns, as follows:
-    with outer_teardown(outer_setup()):
-        outer_work()
-
-        # and let's put section 1 inside the outer setup and teardown tasks
-        section_1()
diff --git a/example_dags/example_short_circuit_decorator.py b/example_dags/example_short_circuit_decorator.py
deleted file mode 100644
index 2d82eee..0000000
--- a/example_dags/example_short_circuit_decorator.py
+++ /dev/null
@@ -1,61 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the `@task.short_circuit()` TaskFlow decorator."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import dag, task
-from airflow.models.baseoperator import chain
-from airflow.operators.empty import EmptyOperator
-from airflow.utils.trigger_rule import TriggerRule
-
-
-@dag(schedule=None, start_date=pendulum.datetime(2021, 1, 1, tz="UTC"), catchup=False, tags=["example"])
-def example_short_circuit_decorator():
-    # [START howto_operator_short_circuit]
-    @task.short_circuit()
-    def check_condition(condition):
-        return condition
-
-    ds_true = [EmptyOperator(task_id=f"true_{i}") for i in [1, 2]]
-    ds_false = [EmptyOperator(task_id=f"false_{i}") for i in [1, 2]]
-
-    condition_is_true = check_condition.override(task_id="condition_is_true")(condition=True)
-    condition_is_false = check_condition.override(task_id="condition_is_false")(condition=False)
-
-    chain(condition_is_true, *ds_true)
-    chain(condition_is_false, *ds_false)
-    # [END howto_operator_short_circuit]
-
-    # [START howto_operator_short_circuit_trigger_rules]
-    [task_1, task_2, task_3, task_4, task_5, task_6] = [
-        EmptyOperator(task_id=f"task_{i}") for i in range(1, 7)
-    ]
-
-    task_7 = EmptyOperator(task_id="task_7", trigger_rule=TriggerRule.ALL_DONE)
-
-    short_circuit = check_condition.override(task_id="short_circuit", ignore_downstream_trigger_rules=False)(
-        condition=False
-    )
-
-    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)
-    # [END howto_operator_short_circuit_trigger_rules]
-
-
-example_dag = example_short_circuit_decorator()
diff --git a/example_dags/example_short_circuit_operator.py b/example_dags/example_short_circuit_operator.py
deleted file mode 100644
index 3941ff1..0000000
--- a/example_dags/example_short_circuit_operator.py
+++ /dev/null
@@ -1,67 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the ShortCircuitOperator."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.baseoperator import chain
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.operators.python import ShortCircuitOperator
-from airflow.utils.trigger_rule import TriggerRule
-
-with DAG(
-    dag_id="example_short_circuit_operator",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-):
-    # [START howto_operator_short_circuit]
-    cond_true = ShortCircuitOperator(
-        task_id="condition_is_True",
-        python_callable=lambda: True,
-    )
-
-    cond_false = ShortCircuitOperator(
-        task_id="condition_is_False",
-        python_callable=lambda: False,
-    )
-
-    ds_true = [EmptyOperator(task_id=f"true_{i}") for i in [1, 2]]
-    ds_false = [EmptyOperator(task_id=f"false_{i}") for i in [1, 2]]
-
-    chain(cond_true, *ds_true)
-    chain(cond_false, *ds_false)
-    # [END howto_operator_short_circuit]
-
-    # [START howto_operator_short_circuit_trigger_rules]
-    [task_1, task_2, task_3, task_4, task_5, task_6] = [
-        EmptyOperator(task_id=f"task_{i}") for i in range(1, 7)
-    ]
-
-    task_7 = EmptyOperator(task_id="task_7", trigger_rule=TriggerRule.ALL_DONE)
-
-    short_circuit = ShortCircuitOperator(
-        task_id="short_circuit", ignore_downstream_trigger_rules=False, python_callable=lambda: False
-    )
-
-    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)
-    # [END howto_operator_short_circuit_trigger_rules]
diff --git a/example_dags/example_skip_dag.py b/example_dags/example_skip_dag.py
deleted file mode 100644
index 2655394..0000000
--- a/example_dags/example_skip_dag.py
+++ /dev/null
@@ -1,73 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the EmptyOperator and a custom EmptySkipOperator which skips by default."""
-
-from __future__ import annotations
-
-import datetime
-from typing import TYPE_CHECKING
-
-import pendulum
-
-from airflow.exceptions import AirflowSkipException
-from airflow.models.baseoperator import BaseOperator
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.utils.trigger_rule import TriggerRule
-
-if TYPE_CHECKING:
-    from airflow.utils.context import Context
-
-
-# Create some placeholder operators
-class EmptySkipOperator(BaseOperator):
-    """Empty operator which always skips the task."""
-
-    ui_color = "#e8b7e4"
-
-    def execute(self, context: Context):
-        raise AirflowSkipException
-
-
-def create_test_pipeline(suffix, trigger_rule):
-    """
-    Instantiate a number of operators for the given DAG.
-
-    :param str suffix: Suffix to append to the operator task_ids
-    :param str trigger_rule: TriggerRule for the join task
-    :param DAG dag_: The DAG to run the operators on
-    """
-    skip_operator = EmptySkipOperator(task_id=f"skip_operator_{suffix}")
-    always_true = EmptyOperator(task_id=f"always_true_{suffix}")
-    join = EmptyOperator(task_id=trigger_rule, trigger_rule=trigger_rule)
-    final = EmptyOperator(task_id=f"final_{suffix}")
-
-    skip_operator >> join
-    always_true >> join
-    join >> final
-
-
-with DAG(
-    dag_id="example_skip_dag",
-    schedule=datetime.timedelta(days=1),
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    create_test_pipeline("1", TriggerRule.ALL_SUCCESS)
-    create_test_pipeline("2", TriggerRule.ONE_SUCCESS)
diff --git a/example_dags/example_task_group.py b/example_dags/example_task_group.py
deleted file mode 100644
index 6435a91..0000000
--- a/example_dags/example_task_group.py
+++ /dev/null
@@ -1,66 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the TaskGroup."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-from airflow.operators.empty import EmptyOperator
-from airflow.utils.task_group import TaskGroup
-
-# [START howto_task_group]
-with DAG(
-    dag_id="example_task_group",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    start = EmptyOperator(task_id="start")
-
-    # [START howto_task_group_section_1]
-    with TaskGroup("section_1", tooltip="Tasks for section_1") as section_1:
-        task_1 = EmptyOperator(task_id="task_1")
-        task_2 = BashOperator(task_id="task_2", bash_command="echo 1")
-        task_3 = EmptyOperator(task_id="task_3")
-
-        task_1 >> [task_2, task_3]
-    # [END howto_task_group_section_1]
-
-    # [START howto_task_group_section_2]
-    with TaskGroup("section_2", tooltip="Tasks for section_2") as section_2:
-        task_1 = EmptyOperator(task_id="task_1")
-
-        # [START howto_task_group_inner_section_2]
-        with TaskGroup("inner_section_2", tooltip="Tasks for inner_section2") as inner_section_2:
-            task_2 = BashOperator(task_id="task_2", bash_command="echo 1")
-            task_3 = EmptyOperator(task_id="task_3")
-            task_4 = EmptyOperator(task_id="task_4")
-
-            [task_2, task_3] >> task_4
-        # [END howto_task_group_inner_section_2]
-
-    # [END howto_task_group_section_2]
-
-    end = EmptyOperator(task_id="end")
-
-    start >> section_1 >> section_2 >> end
-# [END howto_task_group]
diff --git a/example_dags/example_task_group_decorator.py b/example_dags/example_task_group_decorator.py
deleted file mode 100644
index ce4a0e3..0000000
--- a/example_dags/example_task_group_decorator.py
+++ /dev/null
@@ -1,81 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the @taskgroup decorator."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import task, task_group
-from airflow.models.dag import DAG
-
-
-# [START howto_task_group_decorator]
-# Creating Tasks
-@task
-def task_start():
-    """Empty Task which is First Task of Dag"""
-    return "[Task_start]"
-
-
-@task
-def task_1(value: int) -> str:
-    """Empty Task1"""
-    return f"[ Task1 {value} ]"
-
-
-@task
-def task_2(value: str) -> str:
-    """Empty Task2"""
-    return f"[ Task2 {value} ]"
-
-
-@task
-def task_3(value: str) -> None:
-    """Empty Task3"""
-    print(f"[ Task3 {value} ]")
-
-
-@task
-def task_end() -> None:
-    """Empty Task which is Last Task of Dag"""
-    print("[ Task_End  ]")
-
-
-# Creating TaskGroups
-@task_group
-def task_group_function(value: int) -> None:
-    """TaskGroup for grouping related Tasks"""
-    task_3(task_2(task_1(value)))
-
-
-# Executing Tasks and TaskGroups
-with DAG(
-    dag_id="example_task_group_decorator",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    start_task = task_start()
-    end_task = task_end()
-    for i in range(5):
-        current_task_group = task_group_function(i)
-        start_task >> current_task_group >> end_task
-
-# [END howto_task_group_decorator]
diff --git a/example_dags/example_time_delta_sensor_async.py b/example_dags/example_time_delta_sensor_async.py
deleted file mode 100644
index 15a1e52..0000000
--- a/example_dags/example_time_delta_sensor_async.py
+++ /dev/null
@@ -1,42 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example DAG demonstrating ``TimeDeltaSensorAsync``, a drop in replacement for ``TimeDeltaSensor`` that
-defers and doesn't occupy a worker slot while it waits
-"""
-
-from __future__ import annotations
-
-import datetime
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-from airflow.providers.standard.sensors.time_delta import TimeDeltaSensorAsync
-
-with DAG(
-    dag_id="example_time_delta_sensor_async",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    wait = TimeDeltaSensorAsync(task_id="wait", delta=datetime.timedelta(seconds=30))
-    finish = EmptyOperator(task_id="finish")
-    wait >> finish
diff --git a/example_dags/example_trigger_controller_dag.py b/example_dags/example_trigger_controller_dag.py
deleted file mode 100644
index e68e60c..0000000
--- a/example_dags/example_trigger_controller_dag.py
+++ /dev/null
@@ -1,42 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example usage of the TriggerDagRunOperator. This example holds 2 DAGs:
-1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG
-2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.models.dag import DAG
-from airflow.operators.trigger_dagrun import TriggerDagRunOperator
-
-with DAG(
-    dag_id="example_trigger_controller_dag",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    schedule="@once",
-    tags=["example"],
-) as dag:
-    trigger = TriggerDagRunOperator(
-        task_id="test_trigger_dagrun",
-        trigger_dag_id="example_trigger_target_dag",  # Ensure this equals the dag_id of the DAG to trigger
-        conf={"message": "Hello World"},
-    )
diff --git a/example_dags/example_trigger_target_dag.py b/example_dags/example_trigger_target_dag.py
deleted file mode 100644
index 7a009b8..0000000
--- a/example_dags/example_trigger_target_dag.py
+++ /dev/null
@@ -1,56 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-Example usage of the TriggerDagRunOperator. This example holds 2 DAGs:
-1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG
-2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG
-"""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-
-
-@task(task_id="run_this")
-def run_this_func(dag_run=None):
-    """
-    Print the payload "message" passed to the DagRun conf attribute.
-
-    :param dag_run: The DagRun object
-    """
-    print(f"Remotely received value of {dag_run.conf.get('message')} for key=message")
-
-
-with DAG(
-    dag_id="example_trigger_target_dag",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    schedule=None,
-    tags=["example"],
-) as dag:
-    run_this = run_this_func()
-
-    bash_task = BashOperator(
-        task_id="bash_task",
-        bash_command='echo "Here is the message: $message"',
-        env={"message": '{{ dag_run.conf.get("message") }}'},
-    )
diff --git a/example_dags/example_workday_timetable.py b/example_dags/example_workday_timetable.py
deleted file mode 100644
index 8fd86ad..0000000
--- a/example_dags/example_workday_timetable.py
+++ /dev/null
@@ -1,31 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-import pendulum
-
-from airflow.example_dags.plugins.workday import AfterWorkdayTimetable
-from airflow.models.dag import DAG
-from airflow.operators.empty import EmptyOperator
-
-with DAG(
-    dag_id="example_workday_timetable",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    schedule=AfterWorkdayTimetable(),
-    tags=["example", "timetable"],
-):
-    EmptyOperator(task_id="run_this")
diff --git a/example_dags/example_xcom.py b/example_dags/example_xcom.py
deleted file mode 100644
index fa99b91..0000000
--- a/example_dags/example_xcom.py
+++ /dev/null
@@ -1,95 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of XComs."""
-
-from __future__ import annotations
-
-import pendulum
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.models.xcom_arg import XComArg
-from airflow.operators.bash import BashOperator
-
-value_1 = [1, 2, 3]
-value_2 = {"a": "b"}
-
-
-@task
-def push(ti=None):
-    """Pushes an XCom without a specific target"""
-    ti.xcom_push(key="value from pusher 1", value=value_1)
-
-
-@task
-def push_by_returning():
-    """Pushes an XCom without a specific target, just by returning it"""
-    return value_2
-
-
-def _compare_values(pulled_value, check_value):
-    if pulled_value != check_value:
-        raise ValueError(f"The two values differ {pulled_value} and {check_value}")
-
-
-@task
-def puller(pulled_value_2, ti=None):
-    """Pull all previously pushed XComs and check if the pushed values match the pulled values."""
-    pulled_value_1 = ti.xcom_pull(task_ids="push", key="value from pusher 1")
-
-    _compare_values(pulled_value_1, value_1)
-    _compare_values(pulled_value_2, value_2)
-
-
-@task
-def pull_value_from_bash_push(ti=None):
-    bash_pushed_via_return_value = ti.xcom_pull(key="return_value", task_ids="bash_push")
-    bash_manually_pushed_value = ti.xcom_pull(key="manually_pushed_value", task_ids="bash_push")
-    print(f"The xcom value pushed by task push via return value is {bash_pushed_via_return_value}")
-    print(f"The xcom value pushed by task push manually is {bash_manually_pushed_value}")
-
-
-with DAG(
-    "example_xcom",
-    schedule="@once",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    bash_push = BashOperator(
-        task_id="bash_push",
-        bash_command='echo "bash_push demo"  && '
-        'echo "Manually set xcom value '
-        '{{ ti.xcom_push(key="manually_pushed_value", value="manually_pushed_value") }}" && '
-        'echo "value_by_return"',
-    )
-
-    bash_pull = BashOperator(
-        task_id="bash_pull",
-        bash_command='echo "bash pull demo" && '
-        f'echo "The xcom pushed manually is {XComArg(bash_push, key="manually_pushed_value")}" && '
-        f'echo "The returned_value xcom is {XComArg(bash_push)}" && '
-        'echo "finished"',
-        do_xcom_push=False,
-    )
-
-    python_pull_from_bash = pull_value_from_bash_push()
-
-    [bash_pull, python_pull_from_bash] << bash_push
-
-    puller(push_by_returning()) << push()
diff --git a/example_dags/example_xcomargs.py b/example_dags/example_xcomargs.py
deleted file mode 100644
index d9d0c94..0000000
--- a/example_dags/example_xcomargs.py
+++ /dev/null
@@ -1,66 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Example DAG demonstrating the usage of the XComArgs."""
-
-from __future__ import annotations
-
-import logging
-
-import pendulum
-
-from airflow.decorators import task
-from airflow.models.dag import DAG
-from airflow.operators.bash import BashOperator
-
-log = logging.getLogger(__name__)
-
-
-@task
-def generate_value():
-    """Empty function"""
-    return "Bring me a shrubbery!"
-
-
-@task
-def print_value(value, ts=None):
-    """Empty function"""
-    log.info("The knights of Ni say: %s (at %s)", value, ts)
-
-
-with DAG(
-    dag_id="example_xcom_args",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    schedule=None,
-    tags=["example"],
-) as dag:
-    print_value(generate_value())
-
-with DAG(
-    "example_xcom_args_with_operators",
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    schedule=None,
-    tags=["example"],
-) as dag2:
-    bash_op1 = BashOperator(task_id="c", bash_command="echo c")
-    bash_op2 = BashOperator(task_id="d", bash_command="echo c")
-    xcom_args_a = print_value("first!")
-    xcom_args_b = print_value("second!")
-
-    bash_op1 >> xcom_args_a >> xcom_args_b >> bash_op2
diff --git a/example_dags/libs/__init__.py b/example_dags/libs/__init__.py
deleted file mode 100644
index 217e5db..0000000
--- a/example_dags/libs/__init__.py
+++ /dev/null
@@ -1,17 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
diff --git a/example_dags/libs/helper.py b/example_dags/libs/helper.py
deleted file mode 100644
index e6c2e3c..0000000
--- a/example_dags/libs/helper.py
+++ /dev/null
@@ -1,22 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-
-def print_stuff():
-    print("annotated!")
diff --git a/example_dags/plugins/__init__.py b/example_dags/plugins/__init__.py
deleted file mode 100644
index 13a8339..0000000
--- a/example_dags/plugins/__init__.py
+++ /dev/null
@@ -1,16 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
diff --git a/example_dags/plugins/decreasing_priority_weight_strategy.py b/example_dags/plugins/decreasing_priority_weight_strategy.py
deleted file mode 100644
index 3335b7d..0000000
--- a/example_dags/plugins/decreasing_priority_weight_strategy.py
+++ /dev/null
@@ -1,41 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-from typing import TYPE_CHECKING
-
-from airflow.plugins_manager import AirflowPlugin
-from airflow.task.priority_strategy import PriorityWeightStrategy
-
-if TYPE_CHECKING:
-    from airflow.models import TaskInstance
-
-
-# [START custom_priority_weight_strategy]
-class DecreasingPriorityStrategy(PriorityWeightStrategy):
-    """A priority weight strategy that decreases the priority weight with each attempt of the DAG task."""
-
-    def get_weight(self, ti: TaskInstance):
-        return max(3 - ti.try_number + 1, 1)
-
-
-class DecreasingPriorityWeightStrategyPlugin(AirflowPlugin):
-    name = "decreasing_priority_weight_strategy_plugin"
-    priority_weight_strategies = [DecreasingPriorityStrategy]
-
-
-# [END custom_priority_weight_strategy]
diff --git a/example_dags/plugins/event_listener.py b/example_dags/plugins/event_listener.py
deleted file mode 100644
index 4b9be30..0000000
--- a/example_dags/plugins/event_listener.py
+++ /dev/null
@@ -1,172 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-from __future__ import annotations
-
-from typing import TYPE_CHECKING
-
-from airflow.listeners import hookimpl
-
-if TYPE_CHECKING:
-    from airflow.models.dagrun import DagRun
-    from airflow.models.taskinstance import TaskInstance
-    from airflow.utils.state import TaskInstanceState
-
-
-# [START howto_listen_ti_running_task]
-@hookimpl
-def on_task_instance_running(previous_state: TaskInstanceState, task_instance: TaskInstance, session):
-    """
-    This method is called when task state changes to RUNNING.
-    Through callback, parameters like previous_task_state, task_instance object can be accessed.
-    This will give more information about current task_instance that is running its dag_run,
-    task and dag information.
-    """
-    print("Task instance is in running state")
-    print(" Previous state of the Task instance:", previous_state)
-
-    state: TaskInstanceState = task_instance.state
-    name: str = task_instance.task_id
-    start_date = task_instance.start_date
-
-    dagrun = task_instance.dag_run
-    dagrun_status = dagrun.state
-
-    task = task_instance.task
-
-    if TYPE_CHECKING:
-        assert task
-
-    dag = task.dag
-    dag_name = None
-    if dag:
-        dag_name = dag.dag_id
-    print(f"Current task name:{name} state:{state} start_date:{start_date}")
-    print(f"Dag name:{dag_name} and current dag run status:{dagrun_status}")
-
-
-# [END howto_listen_ti_running_task]
-
-
-# [START howto_listen_ti_success_task]
-@hookimpl
-def on_task_instance_success(previous_state: TaskInstanceState, task_instance: TaskInstance, session):
-    """
-    This method is called when task state changes to SUCCESS.
-    Through callback, parameters like previous_task_state, task_instance object can be accessed.
-    This will give more information about current task_instance that has succeeded its
-    dag_run, task and dag information.
-    """
-    print("Task instance in success state")
-    print(" Previous state of the Task instance:", previous_state)
-
-    dag_id = task_instance.dag_id
-    hostname = task_instance.hostname
-    operator = task_instance.operator
-
-    dagrun = task_instance.dag_run
-    queued_at = dagrun.queued_at
-    print(f"Dag name:{dag_id} queued_at:{queued_at}")
-    print(f"Task hostname:{hostname} operator:{operator}")
-
-
-# [END howto_listen_ti_success_task]
-
-
-# [START howto_listen_ti_failure_task]
-@hookimpl
-def on_task_instance_failed(
-    previous_state: TaskInstanceState, task_instance: TaskInstance, error: None | str | BaseException, session
-):
-    """
-    This method is called when task state changes to FAILED.
-    Through callback, parameters like previous_task_state, task_instance object can be accessed.
-    This will give more information about current task_instance that has failed its dag_run,
-    task and dag information.
-    """
-    print("Task instance in failure state")
-
-    start_date = task_instance.start_date
-    end_date = task_instance.end_date
-    duration = task_instance.duration
-
-    dagrun = task_instance.dag_run
-
-    task = task_instance.task
-
-    if TYPE_CHECKING:
-        assert task
-
-    dag = task.dag
-
-    print(f"Task start:{start_date} end:{end_date} duration:{duration}")
-    print(f"Task:{task} dag:{dag} dagrun:{dagrun}")
-    if error:
-        print(f"Failure caused by {error}")
-
-
-# [END howto_listen_ti_failure_task]
-
-
-# [START howto_listen_dagrun_success_task]
-@hookimpl
-def on_dag_run_success(dag_run: DagRun, msg: str):
-    """
-    This method is called when dag run state changes to SUCCESS.
-    """
-    print("Dag run in success state")
-    start_date = dag_run.start_date
-    end_date = dag_run.end_date
-
-    print(f"Dag run start:{start_date} end:{end_date}")
-
-
-# [END howto_listen_dagrun_success_task]
-
-
-# [START howto_listen_dagrun_failure_task]
-@hookimpl
-def on_dag_run_failed(dag_run: DagRun, msg: str):
-    """
-    This method is called when dag run state changes to FAILED.
-    """
-    print("Dag run  in failure state")
-    dag_id = dag_run.dag_id
-    run_id = dag_run.run_id
-    external_trigger = dag_run.external_trigger
-
-    print(f"Dag information:{dag_id} Run id: {run_id} external trigger: {external_trigger}")
-    print(f"Failed with message: {msg}")
-
-
-# [END howto_listen_dagrun_failure_task]
-
-
-# [START howto_listen_dagrun_running_task]
-@hookimpl
-def on_dag_run_running(dag_run: DagRun, msg: str):
-    """
-    This method is called when dag run state changes to RUNNING.
-    """
-    print("Dag run  in running state")
-    queued_at = dag_run.queued_at
-    dag_hash_info = dag_run.dag_hash
-
-    print(f"Dag information Queued at: {queued_at} hash info: {dag_hash_info}")
-
-
-# [END howto_listen_dagrun_running_task]
diff --git a/example_dags/plugins/listener_plugin.py b/example_dags/plugins/listener_plugin.py
deleted file mode 100644
index a365d57..0000000
--- a/example_dags/plugins/listener_plugin.py
+++ /dev/null
@@ -1,26 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-from __future__ import annotations
-
-from airflow.example_dags.plugins import event_listener
-from airflow.plugins_manager import AirflowPlugin
-
-
-class MetadataCollectionPlugin(AirflowPlugin):
-    name = "MetadataCollectionPlugin"
-    listeners = [event_listener]
diff --git a/example_dags/plugins/workday.py b/example_dags/plugins/workday.py
deleted file mode 100644
index 90bd80f..0000000
--- a/example_dags/plugins/workday.py
+++ /dev/null
@@ -1,104 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""Plugin to demonstrate timetable registration and accommodate example DAGs."""
-
-from __future__ import annotations
-
-import logging
-from datetime import timedelta
-from typing import TYPE_CHECKING
-
-# [START howto_timetable]
-from pendulum import UTC, Date, DateTime, Time
-
-from airflow.plugins_manager import AirflowPlugin
-from airflow.timetables.base import DagRunInfo, DataInterval, Timetable
-
-if TYPE_CHECKING:
-    from airflow.timetables.base import TimeRestriction
-
-log = logging.getLogger(__name__)
-try:
-    from pandas.tseries.holiday import USFederalHolidayCalendar
-
-    holiday_calendar = USFederalHolidayCalendar()
-except ImportError:
-    log.warning("Could not import pandas. Holidays will not be considered.")
-    holiday_calendar = None  # type: ignore[assignment]
-
-
-class AfterWorkdayTimetable(Timetable):
-    def get_next_workday(self, d: DateTime, incr=1) -> DateTime:
-        next_start = d
-        while True:
-            if next_start.weekday() not in (5, 6):  # not on weekend
-                if holiday_calendar is None:
-                    holidays = set()
-                else:
-                    holidays = holiday_calendar.holidays(start=next_start, end=next_start).to_pydatetime()
-                if next_start not in holidays:
-                    break
-            next_start = next_start.add(days=incr)
-        return next_start
-
-    # [START howto_timetable_infer_manual_data_interval]
-    def infer_manual_data_interval(self, run_after: DateTime) -> DataInterval:
-        start = DateTime.combine((run_after - timedelta(days=1)).date(), Time.min).replace(tzinfo=UTC)
-        # Skip backwards over weekends and holidays to find last run
-        start = self.get_next_workday(start, incr=-1)
-        return DataInterval(start=start, end=(start + timedelta(days=1)))
-
-    # [END howto_timetable_infer_manual_data_interval]
-
-    # [START howto_timetable_next_dagrun_info]
-    def next_dagrun_info(
-        self,
-        *,
-        last_automated_data_interval: DataInterval | None,
-        restriction: TimeRestriction,
-    ) -> DagRunInfo | None:
-        if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.
-            last_start = last_automated_data_interval.start
-            next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min)
-        # Otherwise this is the first ever run on the regular schedule...
-        elif (earliest := restriction.earliest) is None:
-            return None  # No start_date. Don't schedule.
-        elif not restriction.catchup:
-            # If the DAG has catchup=False, today is the earliest to consider.
-            next_start = max(earliest, DateTime.combine(Date.today(), Time.min))
-        elif earliest.time() != Time.min:
-            # If earliest does not fall on midnight, skip to the next day.
-            next_start = DateTime.combine(earliest.date() + timedelta(days=1), Time.min)
-        else:
-            next_start = earliest
-        # Skip weekends and holidays
-        next_start = self.get_next_workday(next_start.replace(tzinfo=UTC))
-
-        if restriction.latest is not None and next_start > restriction.latest:
-            return None  # Over the DAG's scheduled end; don't schedule.
-        return DagRunInfo.interval(start=next_start, end=(next_start + timedelta(days=1)))
-
-    # [END howto_timetable_next_dagrun_info]
-
-
-class WorkdayTimetablePlugin(AirflowPlugin):
-    name = "workday_timetable_plugin"
-    timetables = [AfterWorkdayTimetable]
-
-
-# [END howto_timetable]
diff --git a/example_dags/sql/sample.sql b/example_dags/sql/sample.sql
deleted file mode 100644
index 23af6ab..0000000
--- a/example_dags/sql/sample.sql
+++ /dev/null
@@ -1,24 +0,0 @@
-/*
- Licensed to the Apache Software Foundation (ASF) under one
- or more contributor license agreements.  See the NOTICE file
- distributed with this work for additional information
- regarding copyright ownership.  The ASF licenses this file
- to you under the Apache License, Version 2.0 (the
- "License"); you may not use this file except in compliance
- with the License.  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing,
- software distributed under the License is distributed on an
- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- KIND, either express or implied.  See the License for the
- specific language governing permissions and limitations
- under the License.
-*/
-
-CREATE TABLE Orders (
-    order_id INT PRIMARY KEY,
-    name TEXT,
-    description TEXT
-)
diff --git a/example_dags/tutorial.py b/example_dags/tutorial.py
deleted file mode 100644
index 0e31775..0000000
--- a/example_dags/tutorial.py
+++ /dev/null
@@ -1,127 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-### Tutorial Documentation
-Documentation that goes along with the Airflow tutorial located
-[here](https://airflow.apache.org/tutorial.html)
-"""
-
-from __future__ import annotations
-
-# [START tutorial]
-# [START import_module]
-import textwrap
-from datetime import datetime, timedelta
-
-# The DAG object; we'll need this to instantiate a DAG
-from airflow.models.dag import DAG
-
-# Operators; we need this to operate!
-from airflow.operators.bash import BashOperator
-
-# [END import_module]
-
-
-# [START instantiate_dag]
-with DAG(
-    "tutorial",
-    # [START default_args]
-    # These args will get passed on to each operator
-    # You can override them on a per-task basis during operator initialization
-    default_args={
-        "depends_on_past": False,
-        "email": ["airflow@example.com"],
-        "email_on_failure": False,
-        "email_on_retry": False,
-        "retries": 1,
-        "retry_delay": timedelta(minutes=5),
-        # 'queue': 'bash_queue',
-        # 'pool': 'backfill',
-        # 'priority_weight': 10,
-        # 'end_date': datetime(2016, 1, 1),
-        # 'wait_for_downstream': False,
-        # 'sla': timedelta(hours=2),
-        # 'execution_timeout': timedelta(seconds=300),
-        # 'on_failure_callback': some_function, # or list of functions
-        # 'on_success_callback': some_other_function, # or list of functions
-        # 'on_retry_callback': another_function, # or list of functions
-        # 'sla_miss_callback': yet_another_function, # or list of functions
-        # 'on_skipped_callback': another_function, #or list of functions
-        # 'trigger_rule': 'all_success'
-    },
-    # [END default_args]
-    description="A simple tutorial DAG",
-    schedule=timedelta(days=1),
-    start_date=datetime(2021, 1, 1),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    # [END instantiate_dag]
-
-    # t1, t2 and t3 are examples of tasks created by instantiating operators
-    # [START basic_task]
-    t1 = BashOperator(
-        task_id="print_date",
-        bash_command="date",
-    )
-
-    t2 = BashOperator(
-        task_id="sleep",
-        depends_on_past=False,
-        bash_command="sleep 5",
-        retries=3,
-    )
-    # [END basic_task]
-
-    # [START documentation]
-    t1.doc_md = textwrap.dedent(
-        """\
-    #### Task Documentation
-    You can document your task using the attributes `doc_md` (markdown),
-    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
-    rendered in the UI's Task Instance Details page.
-    ![img](https://imgs.xkcd.com/comics/fixing_problems.png)
-    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)
-    """
-    )
-
-    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR
-    dag.doc_md = """
-    This is a documentation placed anywhere
-    """  # otherwise, type it like this
-    # [END documentation]
-
-    # [START jinja_template]
-    templated_command = textwrap.dedent(
-        """
-    {% for i in range(5) %}
-        echo "{{ ds }}"
-        echo "{{ macros.ds_add(ds, 7)}}"
-    {% endfor %}
-    """
-    )
-
-    t3 = BashOperator(
-        task_id="templated",
-        depends_on_past=False,
-        bash_command=templated_command,
-    )
-    # [END jinja_template]
-
-    t1 >> [t2, t3]
-# [END tutorial]
diff --git a/example_dags/tutorial_dag.py b/example_dags/tutorial_dag.py
deleted file mode 100644
index 553b194..0000000
--- a/example_dags/tutorial_dag.py
+++ /dev/null
@@ -1,136 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-"""
-### DAG Tutorial Documentation
-This DAG is demonstrating an Extract -> Transform -> Load pipeline
-"""
-
-from __future__ import annotations
-
-# [START tutorial]
-# [START import_module]
-import json
-import textwrap
-
-import pendulum
-
-# The DAG object; we'll need this to instantiate a DAG
-from airflow.models.dag import DAG
-
-# Operators; we need this to operate!
-from airflow.operators.python import PythonOperator
-
-# [END import_module]
-
-# [START instantiate_dag]
-with DAG(
-    "tutorial_dag",
-    # [START default_args]
-    # These args will get passed on to each operator
-    # You can override them on a per-task basis during operator initialization
-    default_args={"retries": 2},
-    # [END default_args]
-    description="DAG tutorial",
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-) as dag:
-    # [END instantiate_dag]
-    # [START documentation]
-    dag.doc_md = __doc__
-    # [END documentation]
-
-    # [START extract_function]
-    def extract(**kwargs):
-        ti = kwargs["ti"]
-        data_string = '{"1001": 301.27, "1002": 433.21, "1003": 502.22}'
-        ti.xcom_push("order_data", data_string)
-
-    # [END extract_function]
-
-    # [START transform_function]
-    def transform(**kwargs):
-        ti = kwargs["ti"]
-        extract_data_string = ti.xcom_pull(task_ids="extract", key="order_data")
-        order_data = json.loads(extract_data_string)
-
-        total_order_value = 0
-        for value in order_data.values():
-            total_order_value += value
-
-        total_value = {"total_order_value": total_order_value}
-        total_value_json_string = json.dumps(total_value)
-        ti.xcom_push("total_order_value", total_value_json_string)
-
-    # [END transform_function]
-
-    # [START load_function]
-    def load(**kwargs):
-        ti = kwargs["ti"]
-        total_value_string = ti.xcom_pull(task_ids="transform", key="total_order_value")
-        total_order_value = json.loads(total_value_string)
-
-        print(total_order_value)
-
-    # [END load_function]
-
-    # [START main_flow]
-    extract_task = PythonOperator(
-        task_id="extract",
-        python_callable=extract,
-    )
-    extract_task.doc_md = textwrap.dedent(
-        """\
-    #### Extract task
-    A simple Extract task to get data ready for the rest of the data pipeline.
-    In this case, getting data is simulated by reading from a hardcoded JSON string.
-    This data is then put into xcom, so that it can be processed by the next task.
-    """
-    )
-
-    transform_task = PythonOperator(
-        task_id="transform",
-        python_callable=transform,
-    )
-    transform_task.doc_md = textwrap.dedent(
-        """\
-    #### Transform task
-    A simple Transform task which takes in the collection of order data from xcom
-    and computes the total order value.
-    This computed value is then put into xcom, so that it can be processed by the next task.
-    """
-    )
-
-    load_task = PythonOperator(
-        task_id="load",
-        python_callable=load,
-    )
-    load_task.doc_md = textwrap.dedent(
-        """\
-    #### Load task
-    A simple Load task which takes in the result of the Transform task, by reading it
-    from xcom and instead of saving it to end user review, just prints it out.
-    """
-    )
-
-    extract_task >> transform_task >> load_task
-
-# [END main_flow]
-
-# [END tutorial]
diff --git a/example_dags/tutorial_objectstorage.py b/example_dags/tutorial_objectstorage.py
deleted file mode 100644
index 4660aa3..0000000
--- a/example_dags/tutorial_objectstorage.py
+++ /dev/null
@@ -1,135 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-# [START tutorial]
-# [START import_module]
-import pendulum
-import requests
-
-from airflow.decorators import dag, task
-from airflow.io.path import ObjectStoragePath
-
-# [END import_module]
-
-API = "https://opendata.fmi.fi/timeseries"
-
-aq_fields = {
-    "fmisid": "int32",
-    "time": "datetime64[ns]",
-    "AQINDEX_PT1H_avg": "float64",
-    "PM10_PT1H_avg": "float64",
-    "PM25_PT1H_avg": "float64",
-    "O3_PT1H_avg": "float64",
-    "CO_PT1H_avg": "float64",
-    "SO2_PT1H_avg": "float64",
-    "NO2_PT1H_avg": "float64",
-    "TRSC_PT1H_avg": "float64",
-}
-
-# [START create_object_storage_path]
-base = ObjectStoragePath("s3://aws_default@airflow-tutorial-data/")
-# [END create_object_storage_path]
-
-
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-)
-def tutorial_objectstorage():
-    """
-    ### Object Storage Tutorial Documentation
-    This is a tutorial DAG to showcase the usage of the Object Storage API.
-    Documentation that goes along with the Airflow Object Storage tutorial is
-    located
-    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/objectstorage.html)
-    """
-
-    # [START get_air_quality_data]
-    @task
-    def get_air_quality_data(**kwargs) -> ObjectStoragePath:
-        """
-        #### Get Air Quality Data
-        This task gets air quality data from the Finnish Meteorological Institute's
-        open data API. The data is saved as parquet.
-        """
-        import pandas as pd
-
-        execution_date = kwargs["logical_date"]
-        start_time = kwargs["data_interval_start"]
-
-        params = {
-            "format": "json",
-            "precision": "double",
-            "groupareas": "0",
-            "producer": "airquality_urban",
-            "area": "Uusimaa",
-            "param": ",".join(aq_fields.keys()),
-            "starttime": start_time.isoformat(timespec="seconds"),
-            "endtime": execution_date.isoformat(timespec="seconds"),
-            "tz": "UTC",
-        }
-
-        response = requests.get(API, params=params)
-        response.raise_for_status()
-
-        # ensure the bucket exists
-        base.mkdir(exist_ok=True)
-
-        formatted_date = execution_date.format("YYYYMMDD")
-        path = base / f"air_quality_{formatted_date}.parquet"
-
-        df = pd.DataFrame(response.json()).astype(aq_fields)
-        with path.open("wb") as file:
-            df.to_parquet(file)
-
-        return path
-
-    # [END get_air_quality_data]
-
-    # [START analyze]
-    @task
-    def analyze(path: ObjectStoragePath, **kwargs):
-        """
-        #### Analyze
-        This task analyzes the air quality data, prints the results
-        """
-        import duckdb
-
-        conn = duckdb.connect(database=":memory:")
-        conn.register_filesystem(path.fs)
-        conn.execute(f"CREATE OR REPLACE TABLE airquality_urban AS SELECT * FROM read_parquet('{path}')")
-
-        df2 = conn.execute("SELECT * FROM airquality_urban").fetchdf()
-
-        print(df2.head())
-
-    # [END analyze]
-
-    # [START main_flow]
-    obj_path = get_air_quality_data()
-    analyze(obj_path)
-    # [END main_flow]
-
-
-# [START dag_invocation]
-tutorial_objectstorage()
-# [END dag_invocation]
-# [END tutorial]
diff --git a/example_dags/tutorial_taskflow_api.py b/example_dags/tutorial_taskflow_api.py
deleted file mode 100644
index f41f729..0000000
--- a/example_dags/tutorial_taskflow_api.py
+++ /dev/null
@@ -1,106 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-# [START tutorial]
-# [START import_module]
-import json
-
-import pendulum
-
-from airflow.decorators import dag, task
-
-# [END import_module]
-
-
-# [START instantiate_dag]
-@dag(
-    schedule=None,
-    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
-    catchup=False,
-    tags=["example"],
-)
-def tutorial_taskflow_api():
-    """
-    ### TaskFlow API Tutorial Documentation
-    This is a simple data pipeline example which demonstrates the use of
-    the TaskFlow API using three simple tasks for Extract, Transform, and Load.
-    Documentation that goes along with the Airflow TaskFlow API tutorial is
-    located
-    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)
-    """
-    # [END instantiate_dag]
-
-    # [START extract]
-    @task()
-    def extract():
-        """
-        #### Extract task
-        A simple Extract task to get data ready for the rest of the data
-        pipeline. In this case, getting data is simulated by reading from a
-        hardcoded JSON string.
-        """
-        data_string = '{"1001": 301.27, "1002": 433.21, "1003": 502.22}'
-
-        order_data_dict = json.loads(data_string)
-        return order_data_dict
-
-    # [END extract]
-
-    # [START transform]
-    @task(multiple_outputs=True)
-    def transform(order_data_dict: dict):
-        """
-        #### Transform task
-        A simple Transform task which takes in the collection of order data and
-        computes the total order value.
-        """
-        total_order_value = 0
-
-        for value in order_data_dict.values():
-            total_order_value += value
-
-        return {"total_order_value": total_order_value}
-
-    # [END transform]
-
-    # [START load]
-    @task()
-    def load(total_order_value: float):
-        """
-        #### Load task
-        A simple Load task which takes in the result of the Transform task and
-        instead of saving it to end user review, just prints it out.
-        """
-
-        print(f"Total order value is: {total_order_value:.2f}")
-
-    # [END load]
-
-    # [START main_flow]
-    order_data = extract()
-    order_summary = transform(order_data)
-    load(order_summary["total_order_value"])
-    # [END main_flow]
-
-
-# [START dag_invocation]
-tutorial_taskflow_api()
-# [END dag_invocation]
-
-# [END tutorial]
diff --git a/example_dags/tutorial_taskflow_api_virtualenv.py b/example_dags/tutorial_taskflow_api_virtualenv.py
deleted file mode 100644
index 3860876..0000000
--- a/example_dags/tutorial_taskflow_api_virtualenv.py
+++ /dev/null
@@ -1,87 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-from __future__ import annotations
-
-import logging
-from datetime import datetime
-
-from airflow.decorators import dag, task
-from airflow.operators.python import is_venv_installed
-
-log = logging.getLogger(__name__)
-
-if not is_venv_installed():
-    log.warning("The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.")
-else:
-
-    @dag(schedule=None, start_date=datetime(2021, 1, 1), catchup=False, tags=["example"])
-    def tutorial_taskflow_api_virtualenv():
-        """
-        ### TaskFlow API example using virtualenv
-        This is a simple data pipeline example which demonstrates the use of
-        the TaskFlow API using three simple tasks for Extract, Transform, and Load.
-        """
-
-        @task.virtualenv(
-            serializer="dill",  # Use `dill` for advanced serialization.
-            system_site_packages=False,
-            requirements=["funcsigs"],
-        )
-        def extract():
-            """
-            #### Extract task
-            A simple Extract task to get data ready for the rest of the data
-            pipeline. In this case, getting data is simulated by reading from a
-            hardcoded JSON string.
-            """
-            import json
-
-            data_string = '{"1001": 301.27, "1002": 433.21, "1003": 502.22}'
-
-            order_data_dict = json.loads(data_string)
-            return order_data_dict
-
-        @task(multiple_outputs=True)
-        def transform(order_data_dict: dict):
-            """
-            #### Transform task
-            A simple Transform task which takes in the collection of order data and
-            computes the total order value.
-            """
-            total_order_value = 0
-
-            for value in order_data_dict.values():
-                total_order_value += value
-
-            return {"total_order_value": total_order_value}
-
-        @task()
-        def load(total_order_value: float):
-            """
-            #### Load task
-            A simple Load task which takes in the result of the Transform task and
-            instead of saving it to end user review, just prints it out.
-            """
-
-            print(f"Total order value is: {total_order_value:.2f}")
-
-        order_data = extract()
-        order_summary = transform(order_data)
-        load(order_summary["total_order_value"])
-
-    tutorial_dag = tutorial_taskflow_api_virtualenv()
diff --git a/toys/complex_dag_structure_rainbow.py b/toys/complex_dag_structure_rainbow.py
deleted file mode 100644
index b068f34..0000000
--- a/toys/complex_dag_structure_rainbow.py
+++ /dev/null
@@ -1,132 +0,0 @@
-"""
-## Example DAG with a complex structure
-
-This DAG doesn't really do anything other than attempting to look pretty.
-Used to show UI features.
-"""
-
-from airflow.decorators import dag, task_group, task
-from airflow.operators.empty import EmptyOperator
-from airflow.models.baseoperator import chain, chain_linear
-from airflow.utils.edgemodifier import Label
-from airflow.datasets import Dataset
-from pendulum import datetime
-
-from include.rainbow_operators.rainbow_operators import (
-    ExtractFromObjectStorageOperator,
-    TransformOperator,
-    LoadtoDWHOperator,
-    LoadAPItoDWHOperator,
-    TransformReportOperator,
-    PublishReportOperator,
-    SpinUpGPUOperator,
-    TrainProprietaryLLMOperator,
-    TearDownGPUOperator,
-)
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule=[Dataset("s3://in_sales_data"), Dataset("az://in_internal_api")],
-    catchup=False,
-    doc_md=__doc__,
-    dag_display_name=" Rgeboge DAG",  # NEW in Airflow 2.9: Define a display name that can include non-ascii characters
-    # (the dag_id only allows alphanumeric characters, dashes, dots and underscores)
-    tags=["toy", "UI DAG"],
-)
-def complex_dag_structure_rainbow():
-
-    start = EmptyOperator(task_id="start", task_display_name="")
-
-    sales_data_extract = ExtractFromObjectStorageOperator.partial(
-        task_id="sales_data_extract", 
-        task_display_name=" fr Sales!"
-    ).expand(my_param=[1, 2, 3, 4])
-    internal_api_extract = ExtractFromObjectStorageOperator.partial(
-        task_id="internal_api_extract"
-    ).expand(my_param=[1, 2, 3, 4])
-
-    @task.branch
-    def determine_load_type():
-        return "internal_api_load_incremental"
-
-    sales_data_transform = TransformOperator(task_id="sales_data_transform")
-
-    determine_load_type_obj = determine_load_type()
-
-    sales_data_load = LoadtoDWHOperator(task_id="sales_data_load")
-    internal_api_load_full = LoadAPItoDWHOperator(task_id="internal_api_load_full")
-    internal_api_load_incremental = LoadAPItoDWHOperator(
-        task_id="internal_api_load_incremental"
-    )
-
-    @task_group()
-    def sales_data_reporting(a):
-        prepare_report = TransformReportOperator(
-            task_id="prepare_report", trigger_rule="all_done"
-        )
-        publish_report = PublishReportOperator(task_id="publish_report")
-
-        chain(prepare_report, publish_report)
-
-    sales_data_reporting_obj = sales_data_reporting.expand(a=[1, 2, 3, 4, 5, 6])
-
-    @task_group
-    def cre_integration():
-        cre_extract = EmptyOperator(task_id="cre_extract", trigger_rule="all_done")
-        cre_transform = EmptyOperator(task_id="cre_transform")
-        cre_load = EmptyOperator(task_id="cre_load")
-
-        chain(cre_extract, cre_transform, cre_load)
-
-    cre_integration_obj = cre_integration()
-
-    @task_group()
-    def mlops():
-        set_up_cluster = SpinUpGPUOperator(
-            task_id="set_up_cluster", trigger_rule="all_done"
-        )
-        train_model = TrainProprietaryLLMOperator(
-            task_id="train_model", outlets=[Dataset("model_trained")]
-        )
-        tear_down_cluster = TearDownGPUOperator(task_id="tear_down_cluster")
-
-        chain(set_up_cluster, train_model, tear_down_cluster)
-
-        tear_down_cluster.as_teardown(setups=set_up_cluster)
-
-    mlops_obj = mlops()
-
-    end = EmptyOperator(task_id="end", task_display_name="")
-
-    chain(
-        start,
-        sales_data_extract,
-        sales_data_transform,
-        sales_data_load,
-        [sales_data_reporting_obj, cre_integration_obj],
-        end,
-    )
-    chain(
-        start,
-        internal_api_extract,
-        determine_load_type_obj,
-        [internal_api_load_full, internal_api_load_incremental],
-        mlops_obj,
-        end,
-    )
-
-    chain_linear(
-        [sales_data_load, internal_api_load_full],
-        [sales_data_reporting_obj, cre_integration_obj],
-    )
-
-    chain(
-        determine_load_type_obj, Label("additional data"), internal_api_load_incremental
-    )
-    chain(
-        determine_load_type_obj, Label("changed existing data"), internal_api_load_full
-    )
-
-
-complex_dag_structure_rainbow()
diff --git a/toys/toy_auto_pause.py b/toys/toy_auto_pause.py
deleted file mode 100644
index c95dfd8..0000000
--- a/toys/toy_auto_pause.py
+++ /dev/null
@@ -1,20 +0,0 @@
-from airflow.decorators import dag, task
-from pendulum import datetime
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule="0 * * * *",
-    catchup=False,
-    max_consecutive_failed_dag_runs=5,
-    doc_md=__doc__,
-)
-def toy_auto_pause():
-    @task
-    def say_hello() -> None:
-        raise (Exception("Hello, I am an exception!"))
-
-    say_hello()
-
-
-toy_auto_pause()
diff --git a/toys/toy_conditional_dataset_scheduling/downstream1_on_any.py b/toys/toy_conditional_dataset_scheduling/downstream1_on_any.py
deleted file mode 100644
index da5040e..0000000
--- a/toys/toy_conditional_dataset_scheduling/downstream1_on_any.py
+++ /dev/null
@@ -1,37 +0,0 @@
-"""
-## Toy DAG scheduled to run on an update to any of 4 upstream datasets
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-from pendulum import datetime
-
-
-@dag(
-    start_date=datetime(2024, 3, 1),
-    schedule=(
-        Dataset("dataset1")
-        | Dataset("dataset2")
-        | Dataset("dataset3")
-        | Dataset("dataset4")
-    ),  # Runs when any of the datasets are updated
-    # NEW in Airflow 2.9: Use conditional logic to schedule a DAG based on datasets
-    # Use () instead of [] to be able to use conditional dataset scheduling!
-    catchup=False,
-    doc_md=__doc__,
-    tags=["toy", "Conditional Dataset Scheduling"],
-)
-def downstream1_on_any():
-    @task
-    def say_hello() -> None:
-        """
-        Print Hello
-        """
-        import time 
-        time.sleep(10)
-        print("Hello")
-
-    say_hello()
-
-
-downstream1_on_any()
diff --git a/toys/toy_conditional_dataset_scheduling/downstream2_one_in_each_group.py b/toys/toy_conditional_dataset_scheduling/downstream2_one_in_each_group.py
deleted file mode 100644
index 979f767..0000000
--- a/toys/toy_conditional_dataset_scheduling/downstream2_one_in_each_group.py
+++ /dev/null
@@ -1,35 +0,0 @@
-"""
-# Toy DAG scheduled to run on an update to one dataset in each of 2 groups
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-from pendulum import datetime
-
-
-@dag(
-    start_date=datetime(2024, 3, 1),
-    schedule=(
-        (Dataset("dataset1") | Dataset("dataset2"))
-        & (Dataset("dataset3") | Dataset("dataset4"))
-    ),  # Runs when one dataset in each group is updated
-    # NEW in Airflow 2.9: Use conditional logic to schedule a DAG based on datasets
-    # Use () instead of [] to be able to use conditional dataset scheduling!
-    catchup=False,
-    doc_md=__doc__,
-    tags=["toy", "Conditional Dataset Scheduling"],
-)
-def downstream2_one_in_each_group():
-    @task
-    def say_hello() -> None:
-        """
-        Print Hello
-        """
-        import time 
-        time.sleep(10)
-        print("Hello")
-
-    say_hello()
-
-
-downstream2_one_in_each_group()
diff --git a/toys/toy_conditional_dataset_scheduling/downstream3_dataset_and_time_schedule.py b/toys/toy_conditional_dataset_scheduling/downstream3_dataset_and_time_schedule.py
deleted file mode 100644
index a0093c1..0000000
--- a/toys/toy_conditional_dataset_scheduling/downstream3_dataset_and_time_schedule.py
+++ /dev/null
@@ -1,37 +0,0 @@
-"""
-# Toy DAG scheduled to run on a cron schedule and an update to any of 2 upstream datasets
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-from pendulum import datetime
-from airflow.timetables.datasets import DatasetOrTimeSchedule
-from airflow.timetables.trigger import CronTriggerTimetable
-
-
-@dag(
-    start_date=datetime(2024, 3, 1),
-    schedule=DatasetOrTimeSchedule(
-        timetable=CronTriggerTimetable("0 * * * *", timezone="UTC"),
-        datasets=(Dataset("dataset3") | Dataset("dataset4")),
-    ),  # Runs every hour and when either of the datasets are updated
-    # NEW in Airflow 2.9: Schedule a DAG both on time and conditional datasets
-    # Use () instead of [] to be able to use conditional dataset scheduling!
-    catchup=False,
-    doc_md=__doc__,
-    tags=["toy", "Conditional Dataset Scheduling", "DatasetOrTimeSchedule"],
-)
-def toy_downstream3_dataset_and_time_schedule():
-    @task
-    def say_hello() -> None:
-        """
-        Print Hello
-        """
-        import time 
-        time.sleep(10)
-        print("Hello")
-
-    say_hello()
-
-
-toy_downstream3_dataset_and_time_schedule()
diff --git a/toys/toy_conditional_dataset_scheduling/upstream1.py b/toys/toy_conditional_dataset_scheduling/upstream1.py
deleted file mode 100644
index dfdf66e..0000000
--- a/toys/toy_conditional_dataset_scheduling/upstream1.py
+++ /dev/null
@@ -1,31 +0,0 @@
-"""
-# Toy Helper DAG to show conditional dataset scheduling
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-from pendulum import datetime
-
-
-@dag(
-    start_date=datetime(2024, 3, 1),
-    schedule=[Dataset("dataset0")],
-    catchup=False,
-    doc_md=__doc__,
-    tags=["toy", "Conditional Dataset Scheduling"],
-)
-def upstream1():
-    @task(outlets=[Dataset("dataset1")])
-    def update_dataset_1() -> None:
-        """
-        Update the dataset
-        """
-        import time
-
-        time.sleep(30)
-        print("Updating dataset 1")
-
-    update_dataset_1()
-
-
-upstream1()
diff --git a/toys/toy_conditional_dataset_scheduling/upstream2.py b/toys/toy_conditional_dataset_scheduling/upstream2.py
deleted file mode 100644
index 8355a22..0000000
--- a/toys/toy_conditional_dataset_scheduling/upstream2.py
+++ /dev/null
@@ -1,27 +0,0 @@
-"""
-# Toy Helper DAG to show conditional dataset scheduling
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["toy", "Conditional Dataset Scheduling"],
-)
-def upstream2():
-    @task(outlets=[Dataset("dataset2")])
-    def update_dataset_2() -> None:
-        """
-        Update the dataset
-        """
-        print("Updating dataset 2")
-
-    update_dataset_2()
-
-
-upstream2()
diff --git a/toys/toy_conditional_dataset_scheduling/upstream3.py b/toys/toy_conditional_dataset_scheduling/upstream3.py
deleted file mode 100644
index 602e8b0..0000000
--- a/toys/toy_conditional_dataset_scheduling/upstream3.py
+++ /dev/null
@@ -1,27 +0,0 @@
-"""
-# Toy Helper DAG to show conditional dataset scheduling
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["Dataset", "toy", "Conditional Dataset Scheduling"],
-)
-def upstream3():
-    @task(outlets=[Dataset("dataset3")])
-    def update_dataset_3() -> None:
-        """
-        Update the dataset
-        """
-        print("Updating dataset 3")
-
-    update_dataset_3()
-
-
-upstream3()
diff --git a/toys/toy_conditional_dataset_scheduling/upstream4.py b/toys/toy_conditional_dataset_scheduling/upstream4.py
deleted file mode 100644
index 3a47a2f..0000000
--- a/toys/toy_conditional_dataset_scheduling/upstream4.py
+++ /dev/null
@@ -1,27 +0,0 @@
-"""
-# Toy Helper DAG to show conditional dataset scheduling
-"""
-
-from airflow.decorators import dag, task
-from airflow.datasets import Dataset
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["Dataset", "toy", "Conditional Dataset Scheduling"],
-)
-def upstream4():
-    @task(outlets=[Dataset("dataset4")])
-    def update_dataset_4() -> None:
-        """
-        Update the dataset
-        """
-        print("Updating dataset 4")
-
-    update_dataset_4()
-
-
-upstream4()
diff --git a/toys/toy_custom_names_dynamic_tasks_taskflow.py b/toys/toy_custom_names_dynamic_tasks_taskflow.py
deleted file mode 100644
index c4c9029..0000000
--- a/toys/toy_custom_names_dynamic_tasks_taskflow.py
+++ /dev/null
@@ -1,65 +0,0 @@
-"""
-### Toy DAG to show how to use custom names for dynamic tasks - TaskFlowAPI
-
-This DAG queries the fruityvice API for information about all fruits. 
-It then creates a dynamically mapped task printing the sugar content of each fruit, 
-with the dynamically mapped task instances being named after the fruit.
-"""
-
-from airflow.decorators import dag, task
-import requests
-from include.helpers import get_display_fruit
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["Dynamic Task Mapping", "toy"],
-)
-def toy_custom_names_dynamic_tasks_taskflow():
-    @task
-    def get_fruits() -> list[dict]:
-        import random
-
-        rand_int = random.randint(10, 49)
-
-        r = requests.get(f"https://www.fruityvice.com/api/fruit/all").json()
-        r = random.sample(r, rand_int)
-
-        return r
-
-    # NEW in Airflow 2.9: Define custom names for the map index
-    @task(map_index_template="{{ my_mapping_variable }}")
-    def map_fruits(fruit_info: dict):
-
-        fruit_name = fruit_info["name"]
-        sugar_content = fruit_info["nutritions"]["sugar"]
-        calories = fruit_info["nutritions"]["calories"]
-        carbs = fruit_info["nutritions"]["carbohydrates"]
-        protein = fruit_info["nutritions"]["protein"]
-        fat = fruit_info["nutritions"]["fat"]
-
-        print(f"{fruit_name} sugar content: {sugar_content}")
-        print(f"{fruit_name} calories: {calories}")
-        print(f"{fruit_name} carbs: {carbs}")
-        print(f"{fruit_name} protein: {protein}")
-        print(f"{fruit_name} fat: {fat}")
-
-        display_fruit = get_display_fruit(fruit_name)
-
-        # create custom map index
-        from airflow.operators.python import get_current_context
-
-        context = get_current_context()
-        # The map index is added after the task has run, so it can include any computed values
-        # from within the task
-        context["my_mapping_variable"] = (
-            f"{display_fruit} {fruit_name} - {sugar_content}g sugar."
-        )
-
-    map_fruits.expand(fruit_info=get_fruits())
-
-
-toy_custom_names_dynamic_tasks_taskflow()
diff --git a/toys/toy_custom_names_dynamic_tasks_traditional_operators.py b/toys/toy_custom_names_dynamic_tasks_traditional_operators.py
deleted file mode 100644
index b37fac6..0000000
--- a/toys/toy_custom_names_dynamic_tasks_traditional_operators.py
+++ /dev/null
@@ -1,51 +0,0 @@
-"""
-### Toy DAG to show how to use custom names for dynamic tasks - traditional operators
-
-This DAG queries the fruityvice API for information about all fruits. 
-It then creates a dynamically mapped task printing the sugar content of each fruit, 
-with the dynamically mapped task instances being named after the fruit.
-"""
-
-from airflow.decorators import dag, task
-from airflow.models.baseoperator import chain
-from airflow.operators.bash import BashOperator
-import requests
-from pendulum import datetime
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule="@daily",
-    catchup=False,
-    doc_md=__doc__,
-    tags=["Dynamic Task Mapping", "toy"],
-)
-def toy_custom_names_dynamic_tasks_traditional_operators():
-    @task
-    def get_fruits() -> list[dict]:
-
-        r = requests.get(f"https://www.fruityvice.com/api/fruit/all").json()
-
-        for fruit in r:
-            fruit.update(fruit.pop("nutritions"))
-
-        for fruit in r:
-            for k, v in fruit.items():
-                fruit[k] = str(v)
-
-        return r
-
-    get_fruits_obj = get_fruits()
-
-    # NEW in Airflow 2.9: Define custom names for the map index
-    map_fruits = BashOperator.partial(
-        task_id="map_fruits",
-        bash_command='echo "$name sugar content: $sugar" && echo "$name calories: $calories" && echo "$name carbs: $carbohydrates" && echo "$name protein: $protein" && echo "$name fat: $fat"',
-        map_index_template="{{ task.env['name'] }} - {{ task.env['sugar'] }}g sugar",
-        # retrieving the fruit name and sugar from the input dictionary
-    ).expand(env=get_fruits_obj)
-
-    chain(get_fruits_obj, map_fruits)
-
-
-toy_custom_names_dynamic_tasks_traditional_operators()
diff --git a/toys/toy_custom_operator_push_multiple_xcom.py b/toys/toy_custom_operator_push_multiple_xcom.py
deleted file mode 100644
index a7eac17..0000000
--- a/toys/toy_custom_operator_push_multiple_xcom.py
+++ /dev/null
@@ -1,34 +0,0 @@
-"""
-### Toy using a custom operator showing multiple xcom pushing
-"""
-
-from airflow.decorators import dag, task
-from include.toy_helpers.custom_operators import MyBasicMathOperator
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["xcom", "toy"],
-)
-def toy_custom_operator_push_multiple_xcom():
-
-    no_multiple_outputs_task = MyBasicMathOperator(
-        task_id="no_multiple_outputs_task",
-        first_number=23,
-        second_number=19,
-        operation="+",
-    )
-
-    multiple_outputs_task = MyBasicMathOperator(
-        task_id="multiple_outputs_task",
-        first_number=23,
-        second_number=19,
-        operation="+",
-        multiple_outputs=True,  # NEW in Airflow 2.9: set multiple_outputs on the BaseOperator
-    )
-
-
-toy_custom_operator_push_multiple_xcom()
diff --git a/toys/toy_downstream_obj_storage_dataset.py b/toys/toy_downstream_obj_storage_dataset.py
deleted file mode 100644
index f0fd8f1..0000000
--- a/toys/toy_downstream_obj_storage_dataset.py
+++ /dev/null
@@ -1,42 +0,0 @@
-"""
-## Toy DAG to show how to use a Dataset in the ObjectStoragePath class - downstream
-
-As of Airflow 2.9, the ObjectStoragePath class now recognizes the Dataset object.
-Use together with the toy_upstream_obj_storage_dataset DAG.
-"""
-
-from airflow.decorators import dag, task
-from airflow.io.path import ObjectStoragePath
-from pendulum import datetime
-from airflow.datasets import Dataset
-
-MY_DATASET = Dataset("file://include/toy_helpers/my_text_file.txt")
-MY_CONN_ID = None  # connecting to the local file system does not require a connection
-
-
-@dag(
-    start_date=datetime(2024, 3, 1),
-    schedule=[MY_DATASET],
-    catchup=False,
-    doc_md=__doc__,
-    tags=["ObjectStorage", "toy", "Dataset"],
-)
-def toy_downstream_obj_storage_dataset():
-
-    @task
-    def read_text_from_file(my_dataset: Dataset, my_conn_id: str) -> None:
-        """
-        Read contents from the file passed into the ObjectStoragePath class
-        using the Dataset Object.
-        Args:
-            my_dataset (Dataset): The dataset to read from.
-            my_conn_id (str): The connection id to use.
-        """
-        # NEW in 2.9: The ObjectStoragePath class now recognizes the Dataset object
-        contents = ObjectStoragePath(my_dataset, conn_id=my_conn_id).read_text()
-        print(contents)
-
-    read_text_from_file(my_dataset=MY_DATASET, my_conn_id=MY_CONN_ID)
-
-
-toy_downstream_obj_storage_dataset()
diff --git a/toys/toy_dynamic_task_default_index.py b/toys/toy_dynamic_task_default_index.py
deleted file mode 100644
index de56497..0000000
--- a/toys/toy_dynamic_task_default_index.py
+++ /dev/null
@@ -1,47 +0,0 @@
-"""
-### Toy DAG to show default dynamic task indices
-"""
-
-from airflow.decorators import dag, task
-import requests
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["Dynamic Task Mapping", "toy"],
-)
-def toy_dynamic_task_default_index():
-    @task
-    def get_fruits() -> list[dict]:
-        import random
-
-        rand_int = random.randint(10, 49)
-
-        r = requests.get(f"https://www.fruityvice.com/api/fruit/all").json()
-        r = random.sample(r, rand_int)
-
-        return r
-
-    @task
-    def map_fruits(fruit_info: dict):
-
-        fruit_name = fruit_info["name"]
-        sugar_content = fruit_info["nutritions"]["sugar"]
-        calories = fruit_info["nutritions"]["calories"]
-        carbs = fruit_info["nutritions"]["carbohydrates"]
-        protein = fruit_info["nutritions"]["protein"]
-        fat = fruit_info["nutritions"]["fat"]
-
-        print(f"{fruit_name} sugar content: {sugar_content}")
-        print(f"{fruit_name} calories: {calories}")
-        print(f"{fruit_name} carbs: {carbs}")
-        print(f"{fruit_name} protein: {protein}")
-        print(f"{fruit_name} fat: {fat}")
-
-    map_fruits.expand(fruit_info=get_fruits())
-
-
-toy_dynamic_task_default_index()
diff --git a/toys/toy_on_skipped_callback.py b/toys/toy_on_skipped_callback.py
deleted file mode 100644
index 77e4b2f..0000000
--- a/toys/toy_on_skipped_callback.py
+++ /dev/null
@@ -1,80 +0,0 @@
-"""
-### Toy DAG showing the on_skipped_callback
-
-on_skipped_callback: 
-- is executed only if an AirflowSkipException gets raised.
-- Explicitly it is NOT called if a task is not started to be executed because of a preceding branching
-decision in the DAG or a trigger rule which causes execution to skip so that the task execution
-is never scheduled.
-"""
-
-from airflow.decorators import dag, task
-from airflow.exceptions import AirflowSkipException
-from airflow.providers.slack.notifications.slack_notifier import SlackNotifier
-from airflow.operators.empty import EmptyOperator
-from airflow.models.baseoperator import chain
-import time
-
-SLACK_CONNECTION_ID = "slack_conn"
-SLACK_CHANNEL = "alerts"
-SLACK_MESSAGE = """
-Hello! The {{ ti.task_id }} task has been SKIPPED! :wave: 
-Timestamp {{ ts }} and this task finished with the state: {{ ti.state }} :tada:.
-"""
-
-slack_notifier_instantiated = SlackNotifier(
-    slack_conn_id=SLACK_CONNECTION_ID,
-    text=SLACK_MESSAGE,
-    channel=SLACK_CHANNEL,
-)
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["on_skipped_callback"],
-)
-def toy_on_skipped_callback():
-
-    # NEW in Airflow 2.9: on_skipped_callback to fire if an AirflowSkipException is raised
-    # This callback is executed only if an AirflowSkipException gets raised, not if a task is skipped
-    # because of a branching decision or a trigger rule.
-    @task(on_skipped_callback=slack_notifier_instantiated)
-    def task_that_raises_skip_exception():
-        time.sleep(5)
-        raise AirflowSkipException("This task is skipped")
-
-    task_that_raises_skip_exception()
-
-    @task.branch
-    def eins_zwei_oder_drei():
-        import random
-
-        number = random.choice([1, 2, 3])
-        if number == 1:
-            return "task_eins"
-        elif number == 2:
-            return "task_zwei"
-        else:
-            return "task_drei"
-
-    @task(on_skipped_callback=slack_notifier_instantiated)
-    def task_eins():
-        print("Eins")
-
-    @task(on_skipped_callback=slack_notifier_instantiated)
-    def task_zwei():
-        print("Zwei")
-
-    @task(on_skipped_callback=slack_notifier_instantiated)
-    def task_drei():
-        print("Drei")
-
-    end = EmptyOperator(task_id="end", trigger_rule="none_failed_or_skipped")
-
-    chain(eins_zwei_oder_drei(), [task_eins(), task_zwei(), task_drei()], end)
-
-
-toy_on_skipped_callback()
diff --git a/toys/toy_task_duration_page.py b/toys/toy_task_duration_page.py
deleted file mode 100644
index 3c786e6..0000000
--- a/toys/toy_task_duration_page.py
+++ /dev/null
@@ -1,56 +0,0 @@
-"""
-### Toy with tasks of different durations
-
-Use this DAG to showcase the new task duration page in the UI.
-"""
-
-from airflow.decorators import dag, task
-import time
-import random
-from pendulum import datetime
-
-random.seed(42)
-
-
-@dag(
-    start_date=datetime(2024, 1, 1),
-    schedule="@daily",
-    catchup=False,
-    doc_md=__doc__,
-    tags=["UI", "toy"],
-)
-def toy_task_duration_page():
-
-    @task
-    def two_second_plus_delay_task() -> None:
-        delay = random.randint(0, 10)
-        time.sleep(2 + delay)
-
-    @task
-    def five_second_plus_delay_task() -> None:
-        delay = random.randint(0, 10)
-        time.sleep(5 + delay)
-
-    @task
-    def ten_second_plus_delay_task() -> None:
-        delay = random.randint(0, 10)
-        time.sleep(10 + delay)
-
-    @task
-    def twenty_second_plus_delay_task() -> None:
-        delay = random.randint(0, 10)
-        time.sleep(20 + delay)
-
-    @task
-    def thirty_second_plus_delay_task() -> None:
-        delay = random.randint(0, 10)
-        time.sleep(30 + delay)
-
-    two_second_plus_delay_task()
-    five_second_plus_delay_task()
-    ten_second_plus_delay_task()
-    twenty_second_plus_delay_task()
-    thirty_second_plus_delay_task()
-
-
-toy_task_duration_page()
diff --git a/toys/toy_taskflow_bash.py b/toys/toy_taskflow_bash.py
deleted file mode 100644
index c055059..0000000
--- a/toys/toy_taskflow_bash.py
+++ /dev/null
@@ -1,47 +0,0 @@
-"""
-### Toy using @task.bash
-"""
-
-from airflow.decorators import dag, task
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["@task.bash", "toy"],
-)
-def toy_taskflow_bash():
-
-    @task
-    def upstream_task():
-        dog_owner_data = {
-            "names": ["Trevor", "Grant", "Marcy", "Carly", "Philip"],
-            "dogs": [1, 2, 2, 0, 4],
-        }
-
-        return dog_owner_data
-
-    # NEW in Airflow 2.9: task.bash, the string that is returned will be executed as a bash command
-    @task.bash
-    def bash_task(dog_owner_data):
-        names_of_dogless_people = []
-        for name, dog in zip(dog_owner_data["names"], dog_owner_data["dogs"]):
-            if dog < 1:
-                names_of_dogless_people.append(name)
-
-        if names_of_dogless_people:
-            if len(names_of_dogless_people) == 1:
-                return f'echo "{names_of_dogless_people[0]} urgently needs a dog!"'
-            else:
-                names_of_dogless_people_str = " and ".join(names_of_dogless_people)
-                return f'echo "{names_of_dogless_people_str} urgently need a dog!"'
-        else:
-            return f'echo "All good, everyone has at least one dog!"'
-
-    bash_task(dog_owner_data=upstream_task())
-
-
-toy_taskflow_bash()
-
diff --git a/toys/toy_upstream_obj_storage_dataset.py b/toys/toy_upstream_obj_storage_dataset.py
deleted file mode 100644
index 7f49696..0000000
--- a/toys/toy_upstream_obj_storage_dataset.py
+++ /dev/null
@@ -1,42 +0,0 @@
-"""
-## Toy DAG to show how to use a Dataset in the ObjectStoragePath class - upstream
-
-As of Airflow 2.9, the ObjectStoragePath class now recognizes the Dataset object.
-Use together with the toy_downstream_obj_storage_dataset DAG.
-"""
-
-from airflow.decorators import dag, task
-from airflow.io.path import ObjectStoragePath
-from pendulum import datetime
-from airflow.datasets import Dataset
-
-MY_DATASET = Dataset("file://include/toy_helpers/my_text_file.txt")
-MY_CONN_ID = None  # connecting to the local file system does not require a connection
-
-
-@dag(
-    start_date=datetime(2024, 3, 1),
-    schedule="@once",
-    catchup=False,
-    doc_md=__doc__,
-    tags=["ObjectStorage", "toy", "Dataset"],
-)
-def toy_upstream_obj_storage_dataset():
-
-    @task(outlets=[MY_DATASET])
-    def write_to_text_file(my_dataset: Dataset, my_conn_id: str) -> None:
-        """
-        Write to the text file passed into the ObjectStoragePath class
-        using the Dataset Object.
-        Args:
-            my_dataset (Dataset): The dataset to read from.
-            my_conn_id (str): The connection id to use.
-        """
-        # NEW in 2.9: The ObjectStoragePath class now recognizes the Dataset object
-        file_path = ObjectStoragePath(my_dataset, conn_id=my_conn_id)
-        file_path.write_text("This is a test!")
-
-    write_to_text_file(my_dataset=MY_DATASET, my_conn_id=MY_CONN_ID)
-
-
-toy_upstream_obj_storage_dataset()
diff --git a/toys/toy_xcom_big_v_small.py b/toys/toy_xcom_big_v_small.py
deleted file mode 100644
index 1c31845..0000000
--- a/toys/toy_xcom_big_v_small.py
+++ /dev/null
@@ -1,47 +0,0 @@
-"""
-## Toy DAG to show size dependant custom XCom serialization
-
-This DAG pushes two objects to XCom, one below, one above 1000 bytes. 
-It then pulls them and prints their sizes.
-"""
-
-from airflow.decorators import dag, task
-from airflow.models.baseoperator import chain
-
-
-@dag(
-    start_date=None,
-    schedule=None,
-    catchup=False,
-    doc_md=__doc__,
-    tags=["Object Store custom XCom backend", "toy"],
-)
-def toy_xcom_big_vs_small():
-    @task
-    def push_objects(**context) -> None:
-        """Create a small and a big dictionary, print their sizes and push them to XCom."""
-
-        small_obj = {"a": 23}
-        big_obj = {f"key{i}": "x" * 100 for i in range(100)}
-        print(f"Size of small object: {small_obj.__sizeof__()}")
-        print(f"Size of big object: {big_obj.__sizeof__()}")
-
-        context["ti"].xcom_push(key="small_obj", value=small_obj)
-        context["ti"].xcom_push(key="big_obj", value=big_obj)
-
-    @task
-    def pull_objects(**context) -> None:
-        """Pull the small and big dictionaries from XCom and print their sizes."""
-
-        small_obj = context["ti"].xcom_pull(task_ids="push_objects", key="small_obj")
-        big_obj = context["ti"].xcom_pull(task_ids="push_objects", key="big_obj")
-
-        print(small_obj)
-        print(f"Size of small object: {small_obj.__sizeof__()}")
-        print(big_obj)
-        print(f"Size of big object: {big_obj.__sizeof__()}")
-
-    chain(push_objects(), pull_objects())
-
-
-toy_xcom_big_vs_small()
